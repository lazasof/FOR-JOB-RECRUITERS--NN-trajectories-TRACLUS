{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Representative Trajectories\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cUaYnERz5tTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import interp1d\n",
        "from scipy.interpolate import interp2d\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "path0='/content/rtr0.csv'\n",
        "path1='/content/rtr1.csv'\n",
        "path2='/content/rtr2.csv'\n",
        "\n",
        "data0 = pd.read_csv(path0, index_col=0)\n",
        "df0=pd.DataFrame({'newlo': data0['newlo'],'newla': data0['newla']})\n",
        "data1 = pd.read_csv(path1, index_col=0)\n",
        "df1=pd.DataFrame({'newlo': data1['newlo'],'newla': data1['newla']})\n",
        "data2 = pd.read_csv(path2, index_col=0)\n",
        "df2=pd.DataFrame({'newlo': data2['newlo'],'newla': data2['newla']})\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rtr0_f=interp1d(df0['newlo'],df0['newla'],'linear',fill_value='extrapolate')\n",
        "plt.scatter(df0['newlo'],df0['newla'])\n",
        "plt.plot(df0['newlo'],rtr0_f(df0['newlo']))\n",
        "\n",
        "rtr1_f=interp1d(df1['newlo'],df1['newla'],'linear',fill_value='extrapolate')\n",
        "plt.scatter(df1['newlo'],df1['newla'])\n",
        "plt.plot(df1['newlo'],rtr1_f(df1['newlo']))\n",
        "\n",
        "\n",
        "rtr2_f=interp1d(df2['newlo'],df2['newla'],'linear',fill_value='extrapolate')\n",
        "plt.scatter(df2['newlo'],df2['newla'])\n",
        "plt.plot(df2['newlo'],rtr2_f(df2['newlo']))\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "ldRr_KXGQXfo",
        "outputId": "3e21c44e-76ec-4602-ede4-5e8b9478b3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViYRem0o5OjF",
        "outputId": "c84b1a7e-5f01-4607-a50d-8f18a65e926e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Using Clustered Data"
      ],
      "metadata": {
        "id": "B9JWW-kDdYw2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "8Btb6OKTPteF",
        "outputId": "1c62361f-b23c-4e05-aca0-ee2f8467c855"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        mmsi(t-10)  dataset_tr1_val2_test3_augm_0(t-10)  \\\n",
              "10     237001000.0                                  1.0   \n",
              "11     237001000.0                                  1.0   \n",
              "12     237001000.0                                  1.0   \n",
              "13     237001000.0                                  1.0   \n",
              "14     237001000.0                                  1.0   \n",
              "...            ...                                  ...   \n",
              "66001  239981000.0                                  3.0   \n",
              "66002  239981000.0                                  3.0   \n",
              "66003  239981000.0                                  3.0   \n",
              "66004  239981000.0                                  3.0   \n",
              "66005  239981000.0                                  3.0   \n",
              "\n",
              "       dataset_tr1_val2_test3_augm_0_norm(t-10)  id(t-10)  WGS84lon(t-10)  \\\n",
              "10                                          1.0       1.0       25.301130   \n",
              "11                                          1.0       1.0       25.288170   \n",
              "12                                          1.0       1.0       25.274370   \n",
              "13                                          1.0       1.0       25.260120   \n",
              "14                                          1.0       1.0       25.239599   \n",
              "...                                         ...       ...             ...   \n",
              "66001                                       3.0     839.0       24.927429   \n",
              "66002                                       3.0     839.0       24.933451   \n",
              "66003                                       3.0     839.0       24.938900   \n",
              "66004                                       3.0     839.0       24.944099   \n",
              "66005                                       3.0     839.0       24.949329   \n",
              "\n",
              "       WGS84lat(t-10)       t(t-10)      lon(t-10)     lat(t-10)  \\\n",
              "10          37.469120  1.516105e+09  349769.161257  4.148271e+06   \n",
              "11          37.473450  1.516105e+09  348631.756210  4.148772e+06   \n",
              "12          37.478050  1.516105e+09  347420.753059  4.149304e+06   \n",
              "13          37.482769  1.516105e+09  346170.352873  4.149851e+06   \n",
              "14          37.489300  1.516105e+09  344369.415740  4.150610e+06   \n",
              "...               ...           ...            ...           ...   \n",
              "66001       37.532120  1.516397e+09  316873.402193  4.155923e+06   \n",
              "66002       37.524151  1.516397e+09  317386.113034  4.155027e+06   \n",
              "66003       37.516972  1.516397e+09  317850.216394  4.154220e+06   \n",
              "66004       37.510052  1.516397e+09  318292.944454  4.153442e+06   \n",
              "66005       37.503052  1.516397e+09  318738.306698  4.152655e+06   \n",
              "\n",
              "       dist_m(t-10)  ...  WGS84lon(t)  WGS84lat(t)        t(t)         lon(t)  \\\n",
              "10      1219.755991  ...    25.161070    37.523739  1516105956  337501.043594   \n",
              "11      1242.934386  ...    25.153830    37.534382  1516106142  336884.403651   \n",
              "12      1322.977531  ...    25.156450    37.536652  1516106323  337120.849949   \n",
              "13      1364.728588  ...    25.156349    37.537781  1516106504  337114.382547   \n",
              "14      1954.106744  ...    25.145929    37.530548  1516107706  336177.869837   \n",
              "...             ...  ...          ...          ...         ...            ...   \n",
              "66001   1425.053560  ...    24.962561    37.455780  1516398151  319794.487192   \n",
              "66002   1032.296525  ...    24.958830    37.444279  1516398356  319436.797993   \n",
              "66003    931.078009  ...    24.953630    37.438580  1516398487  318963.014609   \n",
              "66004    895.072879  ...    24.947229    37.435699  1516398618  318389.705915   \n",
              "66005    904.121257  ...    24.943529    37.438801  1516398757  318069.840146   \n",
              "\n",
              "             lat(t)    dist_m(t)      dlon(t)      dlat(t)   dt(t)  speed(t)  \n",
              "10     4.154564e+06  1791.402528 -1293.225258  1239.633595   185.0  9.683257  \n",
              "11     4.155757e+06  1343.355222  -616.639943  1193.464886   186.0  7.222340  \n",
              "12     4.156004e+06   342.169666   236.446298   247.332224   181.0  1.890440  \n",
              "13     4.156130e+06   125.613514    -6.467402   125.446912   181.0  0.693997  \n",
              "14     4.155345e+06  1221.648328  -936.512710  -784.454321  1202.0  1.016346  \n",
              "...             ...          ...          ...          ...     ...       ...  \n",
              "66001  4.147384e+06   994.046000   -62.082231  -992.105461   132.0  7.530652  \n",
              "66002  4.146115e+06  1318.453403  -357.689199 -1269.006625   205.0  6.431480  \n",
              "66003  4.145493e+06   782.194506  -473.783384  -622.380551   131.0  5.970950  \n",
              "66004  4.145186e+06   650.499941  -573.308693  -307.355356   131.0  4.965648  \n",
              "66005  4.145537e+06   475.134302  -319.865769   351.338148   139.0  3.418232  \n",
              "\n",
              "[57616 rows x 154 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01e203d7-e97d-4218-b786-7d889efb3c43\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mmsi(t-10)</th>\n",
              "      <th>dataset_tr1_val2_test3_augm_0(t-10)</th>\n",
              "      <th>dataset_tr1_val2_test3_augm_0_norm(t-10)</th>\n",
              "      <th>id(t-10)</th>\n",
              "      <th>WGS84lon(t-10)</th>\n",
              "      <th>WGS84lat(t-10)</th>\n",
              "      <th>t(t-10)</th>\n",
              "      <th>lon(t-10)</th>\n",
              "      <th>lat(t-10)</th>\n",
              "      <th>dist_m(t-10)</th>\n",
              "      <th>...</th>\n",
              "      <th>WGS84lon(t)</th>\n",
              "      <th>WGS84lat(t)</th>\n",
              "      <th>t(t)</th>\n",
              "      <th>lon(t)</th>\n",
              "      <th>lat(t)</th>\n",
              "      <th>dist_m(t)</th>\n",
              "      <th>dlon(t)</th>\n",
              "      <th>dlat(t)</th>\n",
              "      <th>dt(t)</th>\n",
              "      <th>speed(t)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>237001000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.301130</td>\n",
              "      <td>37.469120</td>\n",
              "      <td>1.516105e+09</td>\n",
              "      <td>349769.161257</td>\n",
              "      <td>4.148271e+06</td>\n",
              "      <td>1219.755991</td>\n",
              "      <td>...</td>\n",
              "      <td>25.161070</td>\n",
              "      <td>37.523739</td>\n",
              "      <td>1516105956</td>\n",
              "      <td>337501.043594</td>\n",
              "      <td>4.154564e+06</td>\n",
              "      <td>1791.402528</td>\n",
              "      <td>-1293.225258</td>\n",
              "      <td>1239.633595</td>\n",
              "      <td>185.0</td>\n",
              "      <td>9.683257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>237001000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.288170</td>\n",
              "      <td>37.473450</td>\n",
              "      <td>1.516105e+09</td>\n",
              "      <td>348631.756210</td>\n",
              "      <td>4.148772e+06</td>\n",
              "      <td>1242.934386</td>\n",
              "      <td>...</td>\n",
              "      <td>25.153830</td>\n",
              "      <td>37.534382</td>\n",
              "      <td>1516106142</td>\n",
              "      <td>336884.403651</td>\n",
              "      <td>4.155757e+06</td>\n",
              "      <td>1343.355222</td>\n",
              "      <td>-616.639943</td>\n",
              "      <td>1193.464886</td>\n",
              "      <td>186.0</td>\n",
              "      <td>7.222340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>237001000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.274370</td>\n",
              "      <td>37.478050</td>\n",
              "      <td>1.516105e+09</td>\n",
              "      <td>347420.753059</td>\n",
              "      <td>4.149304e+06</td>\n",
              "      <td>1322.977531</td>\n",
              "      <td>...</td>\n",
              "      <td>25.156450</td>\n",
              "      <td>37.536652</td>\n",
              "      <td>1516106323</td>\n",
              "      <td>337120.849949</td>\n",
              "      <td>4.156004e+06</td>\n",
              "      <td>342.169666</td>\n",
              "      <td>236.446298</td>\n",
              "      <td>247.332224</td>\n",
              "      <td>181.0</td>\n",
              "      <td>1.890440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>237001000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.260120</td>\n",
              "      <td>37.482769</td>\n",
              "      <td>1.516105e+09</td>\n",
              "      <td>346170.352873</td>\n",
              "      <td>4.149851e+06</td>\n",
              "      <td>1364.728588</td>\n",
              "      <td>...</td>\n",
              "      <td>25.156349</td>\n",
              "      <td>37.537781</td>\n",
              "      <td>1516106504</td>\n",
              "      <td>337114.382547</td>\n",
              "      <td>4.156130e+06</td>\n",
              "      <td>125.613514</td>\n",
              "      <td>-6.467402</td>\n",
              "      <td>125.446912</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.693997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>237001000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.239599</td>\n",
              "      <td>37.489300</td>\n",
              "      <td>1.516105e+09</td>\n",
              "      <td>344369.415740</td>\n",
              "      <td>4.150610e+06</td>\n",
              "      <td>1954.106744</td>\n",
              "      <td>...</td>\n",
              "      <td>25.145929</td>\n",
              "      <td>37.530548</td>\n",
              "      <td>1516107706</td>\n",
              "      <td>336177.869837</td>\n",
              "      <td>4.155345e+06</td>\n",
              "      <td>1221.648328</td>\n",
              "      <td>-936.512710</td>\n",
              "      <td>-784.454321</td>\n",
              "      <td>1202.0</td>\n",
              "      <td>1.016346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66001</th>\n",
              "      <td>239981000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>839.0</td>\n",
              "      <td>24.927429</td>\n",
              "      <td>37.532120</td>\n",
              "      <td>1.516397e+09</td>\n",
              "      <td>316873.402193</td>\n",
              "      <td>4.155923e+06</td>\n",
              "      <td>1425.053560</td>\n",
              "      <td>...</td>\n",
              "      <td>24.962561</td>\n",
              "      <td>37.455780</td>\n",
              "      <td>1516398151</td>\n",
              "      <td>319794.487192</td>\n",
              "      <td>4.147384e+06</td>\n",
              "      <td>994.046000</td>\n",
              "      <td>-62.082231</td>\n",
              "      <td>-992.105461</td>\n",
              "      <td>132.0</td>\n",
              "      <td>7.530652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66002</th>\n",
              "      <td>239981000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>839.0</td>\n",
              "      <td>24.933451</td>\n",
              "      <td>37.524151</td>\n",
              "      <td>1.516397e+09</td>\n",
              "      <td>317386.113034</td>\n",
              "      <td>4.155027e+06</td>\n",
              "      <td>1032.296525</td>\n",
              "      <td>...</td>\n",
              "      <td>24.958830</td>\n",
              "      <td>37.444279</td>\n",
              "      <td>1516398356</td>\n",
              "      <td>319436.797993</td>\n",
              "      <td>4.146115e+06</td>\n",
              "      <td>1318.453403</td>\n",
              "      <td>-357.689199</td>\n",
              "      <td>-1269.006625</td>\n",
              "      <td>205.0</td>\n",
              "      <td>6.431480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66003</th>\n",
              "      <td>239981000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>839.0</td>\n",
              "      <td>24.938900</td>\n",
              "      <td>37.516972</td>\n",
              "      <td>1.516397e+09</td>\n",
              "      <td>317850.216394</td>\n",
              "      <td>4.154220e+06</td>\n",
              "      <td>931.078009</td>\n",
              "      <td>...</td>\n",
              "      <td>24.953630</td>\n",
              "      <td>37.438580</td>\n",
              "      <td>1516398487</td>\n",
              "      <td>318963.014609</td>\n",
              "      <td>4.145493e+06</td>\n",
              "      <td>782.194506</td>\n",
              "      <td>-473.783384</td>\n",
              "      <td>-622.380551</td>\n",
              "      <td>131.0</td>\n",
              "      <td>5.970950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66004</th>\n",
              "      <td>239981000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>839.0</td>\n",
              "      <td>24.944099</td>\n",
              "      <td>37.510052</td>\n",
              "      <td>1.516397e+09</td>\n",
              "      <td>318292.944454</td>\n",
              "      <td>4.153442e+06</td>\n",
              "      <td>895.072879</td>\n",
              "      <td>...</td>\n",
              "      <td>24.947229</td>\n",
              "      <td>37.435699</td>\n",
              "      <td>1516398618</td>\n",
              "      <td>318389.705915</td>\n",
              "      <td>4.145186e+06</td>\n",
              "      <td>650.499941</td>\n",
              "      <td>-573.308693</td>\n",
              "      <td>-307.355356</td>\n",
              "      <td>131.0</td>\n",
              "      <td>4.965648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66005</th>\n",
              "      <td>239981000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>839.0</td>\n",
              "      <td>24.949329</td>\n",
              "      <td>37.503052</td>\n",
              "      <td>1.516397e+09</td>\n",
              "      <td>318738.306698</td>\n",
              "      <td>4.152655e+06</td>\n",
              "      <td>904.121257</td>\n",
              "      <td>...</td>\n",
              "      <td>24.943529</td>\n",
              "      <td>37.438801</td>\n",
              "      <td>1516398757</td>\n",
              "      <td>318069.840146</td>\n",
              "      <td>4.145537e+06</td>\n",
              "      <td>475.134302</td>\n",
              "      <td>-319.865769</td>\n",
              "      <td>351.338148</td>\n",
              "      <td>139.0</td>\n",
              "      <td>3.418232</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>57616 rows × 154 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01e203d7-e97d-4218-b786-7d889efb3c43')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-01e203d7-e97d-4218-b786-7d889efb3c43 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-01e203d7-e97d-4218-b786-7d889efb3c43');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "\n",
        "data = pd.read_pickle('/content/drive/MyDrive/t10df_20210719.pkl')\n",
        "\n",
        "dfall = pd.read_csv('/content/drive/MyDrive/dfdist66037minlns4573.csv',index_col=0)\n",
        "grouped=dfall.groupby(dfall['clusterId'])\n",
        "df_0 = grouped.get_group(0).copy()\n",
        "df_1 = grouped.get_group(1).copy()\n",
        "df_2 = grouped.get_group(2).copy()\n",
        "tags_of_0=df_0['tags'].unique()\n",
        "tags_of_1=df_1['tags'].unique()\n",
        "tags_of_2=df_2['tags'].unique()\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_pickle('/content/drive/MyDrive/t10df_20210719.pkl')\n",
        "\n",
        "dfall = pd.read_csv('/content/drive/MyDrive/dfdist66037minlns4573.csv',index_col=0)\n",
        "grouped=dfall.groupby(dfall['clusterId'])\n",
        "df_0 = grouped.get_group(0).copy()\n",
        "df_1 = grouped.get_group(1).copy()\n",
        "df_2 = grouped.get_group(2).copy()\n",
        "tags_of_0=df_0['tags'].unique()\n",
        "tags_of_1=df_1['tags'].unique()\n",
        "tags_of_2=df_2['tags'].unique()\n",
        "\n",
        "df=data.filter(regex=(\"^lon|^lat*\"))\n",
        "df['rtr0diff']=np.nan\n",
        "df['rtr1diff']=np.nan\n",
        "df['rtr2diff']=np.nan\n",
        "df=df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  line=np.array([])\n",
        "  diff0=0\n",
        "  diff1=0\n",
        "  diff2=0\n",
        "  line=df.loc[i]\n",
        "\n",
        "  for j in range(1,22,2):\n",
        "      diff0= diff0+abs(rtr0_f(line[j-1])-line[j])\n",
        "      diff1= diff1+abs(rtr1_f(line[j-1])-line[j])\n",
        "      diff2= diff2+abs(rtr2_f(line[j-1])-line[j])\n",
        "\n",
        "  df.rtr0diff[i]=diff0\n",
        "  df.rtr1diff[i]=diff1\n",
        "  df.rtr2diff[i]=diff2\n",
        "\n",
        "\n",
        "df_selected = df.loc[:,['rtr0diff','rtr1diff','rtr2diff']]\n",
        "min_col = np.argmin(df_selected.values,axis=1)\n",
        "df['cluster']=min_col\n",
        "data['cluster']=min_col\n",
        "data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "YtiYmM-pP_T2",
        "outputId": "4d55e700-6e62-4c27-b717-c7fecd799119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        mmsi(t-10)  dataset_tr1_val2_test3_augm_0(t-10)  \\\n",
              "10     237001000.0                                  1.0   \n",
              "11     237001000.0                                  1.0   \n",
              "12     237001000.0                                  1.0   \n",
              "13     237001000.0                                  1.0   \n",
              "14     237001000.0                                  1.0   \n",
              "...            ...                                  ...   \n",
              "66001  239981000.0                                  3.0   \n",
              "66002  239981000.0                                  3.0   \n",
              "66003  239981000.0                                  3.0   \n",
              "66004  239981000.0                                  3.0   \n",
              "66005  239981000.0                                  3.0   \n",
              "\n",
              "       dataset_tr1_val2_test3_augm_0_norm(t-10)  id(t-10)  WGS84lon(t-10)  \\\n",
              "10                                          1.0       1.0       25.301130   \n",
              "11                                          1.0       1.0       25.288170   \n",
              "12                                          1.0       1.0       25.274370   \n",
              "13                                          1.0       1.0       25.260120   \n",
              "14                                          1.0       1.0       25.239599   \n",
              "...                                         ...       ...             ...   \n",
              "66001                                       3.0     839.0       24.927429   \n",
              "66002                                       3.0     839.0       24.933451   \n",
              "66003                                       3.0     839.0       24.938900   \n",
              "66004                                       3.0     839.0       24.944099   \n",
              "66005                                       3.0     839.0       24.949329   \n",
              "\n",
              "       WGS84lat(t-10)       t(t-10)      lon(t-10)     lat(t-10)  \\\n",
              "10          37.469120  1.516105e+09  349769.161257  4.148271e+06   \n",
              "11          37.473450  1.516105e+09  348631.756210  4.148772e+06   \n",
              "12          37.478050  1.516105e+09  347420.753059  4.149304e+06   \n",
              "13          37.482769  1.516105e+09  346170.352873  4.149851e+06   \n",
              "14          37.489300  1.516105e+09  344369.415740  4.150610e+06   \n",
              "...               ...           ...            ...           ...   \n",
              "66001       37.532120  1.516397e+09  316873.402193  4.155923e+06   \n",
              "66002       37.524151  1.516397e+09  317386.113034  4.155027e+06   \n",
              "66003       37.516972  1.516397e+09  317850.216394  4.154220e+06   \n",
              "66004       37.510052  1.516397e+09  318292.944454  4.153442e+06   \n",
              "66005       37.503052  1.516397e+09  318738.306698  4.152655e+06   \n",
              "\n",
              "       dist_m(t-10)  ...  WGS84lat(t)        t(t)         lon(t)  \\\n",
              "10      1219.755991  ...    37.523739  1516105956  337501.043594   \n",
              "11      1242.934386  ...    37.534382  1516106142  336884.403651   \n",
              "12      1322.977531  ...    37.536652  1516106323  337120.849949   \n",
              "13      1364.728588  ...    37.537781  1516106504  337114.382547   \n",
              "14      1954.106744  ...    37.530548  1516107706  336177.869837   \n",
              "...             ...  ...          ...         ...            ...   \n",
              "66001   1425.053560  ...    37.455780  1516398151  319794.487192   \n",
              "66002   1032.296525  ...    37.444279  1516398356  319436.797993   \n",
              "66003    931.078009  ...    37.438580  1516398487  318963.014609   \n",
              "66004    895.072879  ...    37.435699  1516398618  318389.705915   \n",
              "66005    904.121257  ...    37.438801  1516398757  318069.840146   \n",
              "\n",
              "             lat(t)    dist_m(t)      dlon(t)      dlat(t)   dt(t)  speed(t)  \\\n",
              "10     4.154564e+06  1791.402528 -1293.225258  1239.633595   185.0  9.683257   \n",
              "11     4.155757e+06  1343.355222  -616.639943  1193.464886   186.0  7.222340   \n",
              "12     4.156004e+06   342.169666   236.446298   247.332224   181.0  1.890440   \n",
              "13     4.156130e+06   125.613514    -6.467402   125.446912   181.0  0.693997   \n",
              "14     4.155345e+06  1221.648328  -936.512710  -784.454321  1202.0  1.016346   \n",
              "...             ...          ...          ...          ...     ...       ...   \n",
              "66001  4.147384e+06   994.046000   -62.082231  -992.105461   132.0  7.530652   \n",
              "66002  4.146115e+06  1318.453403  -357.689199 -1269.006625   205.0  6.431480   \n",
              "66003  4.145493e+06   782.194506  -473.783384  -622.380551   131.0  5.970950   \n",
              "66004  4.145186e+06   650.499941  -573.308693  -307.355356   131.0  4.965648   \n",
              "66005  4.145537e+06   475.134302  -319.865769   351.338148   139.0  3.418232   \n",
              "\n",
              "       cluster  \n",
              "10           0  \n",
              "11           0  \n",
              "12           0  \n",
              "13           0  \n",
              "14           0  \n",
              "...        ...  \n",
              "66001        2  \n",
              "66002        2  \n",
              "66003        2  \n",
              "66004        2  \n",
              "66005        2  \n",
              "\n",
              "[57616 rows x 155 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b237e6b6-e845-4b10-b2b0-d4dfe3c8fc2c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mmsi(t-10)</th>\n",
              "      <th>dataset_tr1_val2_test3_augm_0(t-10)</th>\n",
              "      <th>dataset_tr1_val2_test3_augm_0_norm(t-10)</th>\n",
              "      <th>id(t-10)</th>\n",
              "      <th>WGS84lon(t-10)</th>\n",
              "      <th>WGS84lat(t-10)</th>\n",
              "      <th>t(t-10)</th>\n",
              "      <th>lon(t-10)</th>\n",
              "      <th>lat(t-10)</th>\n",
              "      <th>dist_m(t-10)</th>\n",
              "      <th>...</th>\n",
              "      <th>WGS84lat(t)</th>\n",
              "      <th>t(t)</th>\n",
              "      <th>lon(t)</th>\n",
              "      <th>lat(t)</th>\n",
              "      <th>dist_m(t)</th>\n",
              "      <th>dlon(t)</th>\n",
              "      <th>dlat(t)</th>\n",
              "      <th>dt(t)</th>\n",
              "      <th>speed(t)</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>237001000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.301130</td>\n",
              "      <td>37.469120</td>\n",
              "      <td>1.516105e+09</td>\n",
              "      <td>349769.161257</td>\n",
              "      <td>4.148271e+06</td>\n",
              "      <td>1219.755991</td>\n",
              "      <td>...</td>\n",
              "      <td>37.523739</td>\n",
              "      <td>1516105956</td>\n",
              "      <td>337501.043594</td>\n",
              "      <td>4.154564e+06</td>\n",
              "      <td>1791.402528</td>\n",
              "      <td>-1293.225258</td>\n",
              "      <td>1239.633595</td>\n",
              "      <td>185.0</td>\n",
              "      <td>9.683257</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>237001000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.288170</td>\n",
              "      <td>37.473450</td>\n",
              "      <td>1.516105e+09</td>\n",
              "      <td>348631.756210</td>\n",
              "      <td>4.148772e+06</td>\n",
              "      <td>1242.934386</td>\n",
              "      <td>...</td>\n",
              "      <td>37.534382</td>\n",
              "      <td>1516106142</td>\n",
              "      <td>336884.403651</td>\n",
              "      <td>4.155757e+06</td>\n",
              "      <td>1343.355222</td>\n",
              "      <td>-616.639943</td>\n",
              "      <td>1193.464886</td>\n",
              "      <td>186.0</td>\n",
              "      <td>7.222340</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>237001000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.274370</td>\n",
              "      <td>37.478050</td>\n",
              "      <td>1.516105e+09</td>\n",
              "      <td>347420.753059</td>\n",
              "      <td>4.149304e+06</td>\n",
              "      <td>1322.977531</td>\n",
              "      <td>...</td>\n",
              "      <td>37.536652</td>\n",
              "      <td>1516106323</td>\n",
              "      <td>337120.849949</td>\n",
              "      <td>4.156004e+06</td>\n",
              "      <td>342.169666</td>\n",
              "      <td>236.446298</td>\n",
              "      <td>247.332224</td>\n",
              "      <td>181.0</td>\n",
              "      <td>1.890440</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>237001000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.260120</td>\n",
              "      <td>37.482769</td>\n",
              "      <td>1.516105e+09</td>\n",
              "      <td>346170.352873</td>\n",
              "      <td>4.149851e+06</td>\n",
              "      <td>1364.728588</td>\n",
              "      <td>...</td>\n",
              "      <td>37.537781</td>\n",
              "      <td>1516106504</td>\n",
              "      <td>337114.382547</td>\n",
              "      <td>4.156130e+06</td>\n",
              "      <td>125.613514</td>\n",
              "      <td>-6.467402</td>\n",
              "      <td>125.446912</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.693997</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>237001000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.239599</td>\n",
              "      <td>37.489300</td>\n",
              "      <td>1.516105e+09</td>\n",
              "      <td>344369.415740</td>\n",
              "      <td>4.150610e+06</td>\n",
              "      <td>1954.106744</td>\n",
              "      <td>...</td>\n",
              "      <td>37.530548</td>\n",
              "      <td>1516107706</td>\n",
              "      <td>336177.869837</td>\n",
              "      <td>4.155345e+06</td>\n",
              "      <td>1221.648328</td>\n",
              "      <td>-936.512710</td>\n",
              "      <td>-784.454321</td>\n",
              "      <td>1202.0</td>\n",
              "      <td>1.016346</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66001</th>\n",
              "      <td>239981000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>839.0</td>\n",
              "      <td>24.927429</td>\n",
              "      <td>37.532120</td>\n",
              "      <td>1.516397e+09</td>\n",
              "      <td>316873.402193</td>\n",
              "      <td>4.155923e+06</td>\n",
              "      <td>1425.053560</td>\n",
              "      <td>...</td>\n",
              "      <td>37.455780</td>\n",
              "      <td>1516398151</td>\n",
              "      <td>319794.487192</td>\n",
              "      <td>4.147384e+06</td>\n",
              "      <td>994.046000</td>\n",
              "      <td>-62.082231</td>\n",
              "      <td>-992.105461</td>\n",
              "      <td>132.0</td>\n",
              "      <td>7.530652</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66002</th>\n",
              "      <td>239981000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>839.0</td>\n",
              "      <td>24.933451</td>\n",
              "      <td>37.524151</td>\n",
              "      <td>1.516397e+09</td>\n",
              "      <td>317386.113034</td>\n",
              "      <td>4.155027e+06</td>\n",
              "      <td>1032.296525</td>\n",
              "      <td>...</td>\n",
              "      <td>37.444279</td>\n",
              "      <td>1516398356</td>\n",
              "      <td>319436.797993</td>\n",
              "      <td>4.146115e+06</td>\n",
              "      <td>1318.453403</td>\n",
              "      <td>-357.689199</td>\n",
              "      <td>-1269.006625</td>\n",
              "      <td>205.0</td>\n",
              "      <td>6.431480</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66003</th>\n",
              "      <td>239981000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>839.0</td>\n",
              "      <td>24.938900</td>\n",
              "      <td>37.516972</td>\n",
              "      <td>1.516397e+09</td>\n",
              "      <td>317850.216394</td>\n",
              "      <td>4.154220e+06</td>\n",
              "      <td>931.078009</td>\n",
              "      <td>...</td>\n",
              "      <td>37.438580</td>\n",
              "      <td>1516398487</td>\n",
              "      <td>318963.014609</td>\n",
              "      <td>4.145493e+06</td>\n",
              "      <td>782.194506</td>\n",
              "      <td>-473.783384</td>\n",
              "      <td>-622.380551</td>\n",
              "      <td>131.0</td>\n",
              "      <td>5.970950</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66004</th>\n",
              "      <td>239981000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>839.0</td>\n",
              "      <td>24.944099</td>\n",
              "      <td>37.510052</td>\n",
              "      <td>1.516397e+09</td>\n",
              "      <td>318292.944454</td>\n",
              "      <td>4.153442e+06</td>\n",
              "      <td>895.072879</td>\n",
              "      <td>...</td>\n",
              "      <td>37.435699</td>\n",
              "      <td>1516398618</td>\n",
              "      <td>318389.705915</td>\n",
              "      <td>4.145186e+06</td>\n",
              "      <td>650.499941</td>\n",
              "      <td>-573.308693</td>\n",
              "      <td>-307.355356</td>\n",
              "      <td>131.0</td>\n",
              "      <td>4.965648</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66005</th>\n",
              "      <td>239981000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>839.0</td>\n",
              "      <td>24.949329</td>\n",
              "      <td>37.503052</td>\n",
              "      <td>1.516397e+09</td>\n",
              "      <td>318738.306698</td>\n",
              "      <td>4.152655e+06</td>\n",
              "      <td>904.121257</td>\n",
              "      <td>...</td>\n",
              "      <td>37.438801</td>\n",
              "      <td>1516398757</td>\n",
              "      <td>318069.840146</td>\n",
              "      <td>4.145537e+06</td>\n",
              "      <td>475.134302</td>\n",
              "      <td>-319.865769</td>\n",
              "      <td>351.338148</td>\n",
              "      <td>139.0</td>\n",
              "      <td>3.418232</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>57616 rows × 155 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b237e6b6-e845-4b10-b2b0-d4dfe3c8fc2c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b237e6b6-e845-4b10-b2b0-d4dfe3c8fc2c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b237e6b6-e845-4b10-b2b0-d4dfe3c8fc2c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import glob\n",
        "import shutil\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "from pandas import concat\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#pip install -U scikit-learn scipy matplotlib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data['column_mmsi'] = np.where(data[\"mmsi(t-1)\"] == data[\"mmsi(t)\"], True, False)\n",
        "false_count = (~data.column_mmsi).sum()\n",
        "true_count = (data.column_mmsi).sum()\n",
        "print(\"num of false mmsi\",(false_count))\n",
        "print(\"num of true mmsi\",(true_count))\n",
        "\n",
        "data['column_id'] = np.where(data[\"id(t-1)\"] == data[\"id(t)\"], True, False)\n",
        "false_count_id = (~data.column_id).sum()\n",
        "true_count_id = (data.column_id).sum()\n",
        "print(\"num of false id\",(false_count_id))\n",
        "print(\"num of true id\",(true_count_id))\n",
        "\n",
        "#to id dinei to trip (mporw na xrhsimopoihsw ws timegap 1 h)\n",
        "\n",
        "data = data[data['column_mmsi'] == True] #drop rows where column id is false, id(t)=/=id(t-1)\n",
        "data['column_mmsi'] = np.where(data[\"mmsi(t-1)\"] == data[\"mmsi(t)\"], True, False)\n",
        "false_count = (~data.column_mmsi).sum()\n",
        "true_count = (data.column_mmsi).sum()\n",
        "print(\"num of false mmsi\",(false_count))\n",
        "print(\"num of true mmsi\",(true_count))\n",
        "\n",
        "data['column_id'] = np.where(data[\"id(t-1)\"] == data[\"id(t)\"], True, False)\n",
        "false_count_id = (~data.column_id).sum()\n",
        "true_count_id = (data.column_id).sum()\n",
        "print(\"num of false id\",(false_count_id))\n",
        "print(\"num of true id\",(true_count_id))\n",
        "print(\"now mmsi(t) == mmsi(t-1) \")\n",
        "#shift me group, short me xrono\n",
        "\n",
        "df0=data.copy()\n",
        "df0 = df0[df0.columns.drop(list(df0.filter(regex='mmsi')))]\n",
        "df0 = df0[df0.columns.drop(list(df0.filter(regex='dataset_tr1_val2_test3_augm_0_norm')))]\n",
        "df0 = df0[df0.columns.drop(list(df0.filter(regex='id')))]\n",
        "df0 = df0[df0.columns.drop(list(df0.filter(regex='WGS84')))]\n",
        "df0 = df0[df0.columns.drop(list(df0.filter(regex='dist_m')))]\n",
        "\n",
        "\n",
        "df0=df0.drop(labels=['t(t-10)', 't(t-9)','t(t-8)','t(t-7)',\n",
        "                 't(t-6)','t(t-5)','t(t-4)', 't(t-3)','t(t-2)','t(t-1)','t(t)','dt(t-10)'], axis=1) \n",
        "\n",
        "df0=df0.drop(labels=['dataset_tr1_val2_test3_augm_0(t-9)', 'dataset_tr1_val2_test3_augm_0(t-8)',\n",
        "                     'dataset_tr1_val2_test3_augm_0(t-7)','dataset_tr1_val2_test3_augm_0(t-6)',\n",
        "                 'dataset_tr1_val2_test3_augm_0(t-5)','dataset_tr1_val2_test3_augm_0(t-4)',\n",
        "                     'dataset_tr1_val2_test3_augm_0(t-3)', 'dataset_tr1_val2_test3_augm_0(t-2)',\n",
        "                     'dataset_tr1_val2_test3_augm_0(t-1)','dataset_tr1_val2_test3_augm_0(t)'], axis=1) \n",
        "\n",
        "df0\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "6a393YP8Qh8q",
        "outputId": "7bb38c00-4b18-4b86-c39b-112bb39dc5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num of false mmsi 0\n",
            "num of true mmsi 57616\n",
            "num of false id 0\n",
            "num of true id 57616\n",
            "num of false mmsi 0\n",
            "num of true mmsi 57616\n",
            "num of false id 0\n",
            "num of true id 57616\n",
            "now mmsi(t) == mmsi(t-1) \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       dataset_tr1_val2_test3_augm_0(t-10)      lon(t-10)     lat(t-10)  \\\n",
              "10                                     1.0  349769.161257  4.148271e+06   \n",
              "11                                     1.0  348631.756210  4.148772e+06   \n",
              "12                                     1.0  347420.753059  4.149304e+06   \n",
              "13                                     1.0  346170.352873  4.149851e+06   \n",
              "14                                     1.0  344369.415740  4.150610e+06   \n",
              "...                                    ...            ...           ...   \n",
              "66001                                  3.0  316873.402193  4.155923e+06   \n",
              "66002                                  3.0  317386.113034  4.155027e+06   \n",
              "66003                                  3.0  317850.216394  4.154220e+06   \n",
              "66004                                  3.0  318292.944454  4.153442e+06   \n",
              "66005                                  3.0  318738.306698  4.152655e+06   \n",
              "\n",
              "        dlon(t-10)   dlat(t-10)  speed(t-10)       lon(t-9)      lat(t-9)  \\\n",
              "10    -1136.822260   442.085767     8.968794  348631.756210  4.148772e+06   \n",
              "11    -1137.405047   501.194222     9.864559  347420.753059  4.149304e+06   \n",
              "12    -1211.003151   532.673367     9.947199  346170.352873  4.149851e+06   \n",
              "13    -1250.400186   546.793831     9.889338  344369.415740  4.150610e+06   \n",
              "14    -1800.937133   758.392122     9.869226  343196.914810  4.151131e+06   \n",
              "...            ...          ...          ...            ...           ...   \n",
              "66001   701.842849 -1240.239599     7.422154  317386.113034  4.155027e+06   \n",
              "66002   512.710842  -895.970819     7.480410  317850.216394  4.154220e+06   \n",
              "66003   464.103359  -807.164377     7.569740  318292.944454  4.153442e+06   \n",
              "66004   442.728060  -777.912156     7.458941  318738.306698  4.152655e+06   \n",
              "66005   445.362245  -786.821275     7.534344  319187.487089  4.151877e+06   \n",
              "\n",
              "         dlon(t-9)   dlat(t-9)  ...    dlat(t-1)  dt(t-1)  speed(t-1)  \\\n",
              "10    -1137.405047  501.194222  ...   589.802678    126.0    9.708484   \n",
              "11    -1211.003151  532.673367  ...  1239.633595    185.0    9.683257   \n",
              "12    -1250.400186  546.793831  ...  1193.464886    186.0    7.222340   \n",
              "13    -1800.937133  758.392122  ...   247.332224    181.0    1.890440   \n",
              "14    -1172.500929  521.606948  ...   125.446912    181.0    0.693997   \n",
              "...            ...         ...  ...          ...      ...         ...   \n",
              "66001   512.710842 -895.970819  ...  -996.591755    131.0    7.610401   \n",
              "66002   464.103359 -807.164377  ...  -992.105461    132.0    7.530652   \n",
              "66003   442.728060 -777.912156  ... -1269.006625    205.0    6.431480   \n",
              "66004   445.362245 -786.821275  ...  -622.380551    131.0    5.970950   \n",
              "66005   449.180390 -777.993407  ...  -307.355356    131.0    4.965648   \n",
              "\n",
              "              lon(t)        lat(t)      dlon(t)      dlat(t)   dt(t)  \\\n",
              "10     337501.043594  4.154564e+06 -1293.225258  1239.633595   185.0   \n",
              "11     336884.403651  4.155757e+06  -616.639943  1193.464886   186.0   \n",
              "12     337120.849949  4.156004e+06   236.446298   247.332224   181.0   \n",
              "13     337114.382547  4.156130e+06    -6.467402   125.446912   181.0   \n",
              "14     336177.869837  4.155345e+06  -936.512710  -784.454321  1202.0   \n",
              "...              ...           ...          ...          ...     ...   \n",
              "66001  319794.487192  4.147384e+06   -62.082231  -992.105461   132.0   \n",
              "66002  319436.797993  4.146115e+06  -357.689199 -1269.006625   205.0   \n",
              "66003  318963.014609  4.145493e+06  -473.783384  -622.380551   131.0   \n",
              "66004  318389.705915  4.145186e+06  -573.308693  -307.355356   131.0   \n",
              "66005  318069.840146  4.145537e+06  -319.865769   351.338148   139.0   \n",
              "\n",
              "       speed(t)  cluster  \n",
              "10     9.683257        0  \n",
              "11     7.222340        0  \n",
              "12     1.890440        0  \n",
              "13     0.693997        0  \n",
              "14     1.016346        0  \n",
              "...         ...      ...  \n",
              "66001  7.530652        2  \n",
              "66002  6.431480        2  \n",
              "66003  5.970950        2  \n",
              "66004  4.965648        2  \n",
              "66005  3.418232        2  \n",
              "\n",
              "[57616 rows x 67 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4b3f998-ea21-4a88-b3fc-c95a766fa23d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset_tr1_val2_test3_augm_0(t-10)</th>\n",
              "      <th>lon(t-10)</th>\n",
              "      <th>lat(t-10)</th>\n",
              "      <th>dlon(t-10)</th>\n",
              "      <th>dlat(t-10)</th>\n",
              "      <th>speed(t-10)</th>\n",
              "      <th>lon(t-9)</th>\n",
              "      <th>lat(t-9)</th>\n",
              "      <th>dlon(t-9)</th>\n",
              "      <th>dlat(t-9)</th>\n",
              "      <th>...</th>\n",
              "      <th>dlat(t-1)</th>\n",
              "      <th>dt(t-1)</th>\n",
              "      <th>speed(t-1)</th>\n",
              "      <th>lon(t)</th>\n",
              "      <th>lat(t)</th>\n",
              "      <th>dlon(t)</th>\n",
              "      <th>dlat(t)</th>\n",
              "      <th>dt(t)</th>\n",
              "      <th>speed(t)</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.0</td>\n",
              "      <td>349769.161257</td>\n",
              "      <td>4.148271e+06</td>\n",
              "      <td>-1136.822260</td>\n",
              "      <td>442.085767</td>\n",
              "      <td>8.968794</td>\n",
              "      <td>348631.756210</td>\n",
              "      <td>4.148772e+06</td>\n",
              "      <td>-1137.405047</td>\n",
              "      <td>501.194222</td>\n",
              "      <td>...</td>\n",
              "      <td>589.802678</td>\n",
              "      <td>126.0</td>\n",
              "      <td>9.708484</td>\n",
              "      <td>337501.043594</td>\n",
              "      <td>4.154564e+06</td>\n",
              "      <td>-1293.225258</td>\n",
              "      <td>1239.633595</td>\n",
              "      <td>185.0</td>\n",
              "      <td>9.683257</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.0</td>\n",
              "      <td>348631.756210</td>\n",
              "      <td>4.148772e+06</td>\n",
              "      <td>-1137.405047</td>\n",
              "      <td>501.194222</td>\n",
              "      <td>9.864559</td>\n",
              "      <td>347420.753059</td>\n",
              "      <td>4.149304e+06</td>\n",
              "      <td>-1211.003151</td>\n",
              "      <td>532.673367</td>\n",
              "      <td>...</td>\n",
              "      <td>1239.633595</td>\n",
              "      <td>185.0</td>\n",
              "      <td>9.683257</td>\n",
              "      <td>336884.403651</td>\n",
              "      <td>4.155757e+06</td>\n",
              "      <td>-616.639943</td>\n",
              "      <td>1193.464886</td>\n",
              "      <td>186.0</td>\n",
              "      <td>7.222340</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.0</td>\n",
              "      <td>347420.753059</td>\n",
              "      <td>4.149304e+06</td>\n",
              "      <td>-1211.003151</td>\n",
              "      <td>532.673367</td>\n",
              "      <td>9.947199</td>\n",
              "      <td>346170.352873</td>\n",
              "      <td>4.149851e+06</td>\n",
              "      <td>-1250.400186</td>\n",
              "      <td>546.793831</td>\n",
              "      <td>...</td>\n",
              "      <td>1193.464886</td>\n",
              "      <td>186.0</td>\n",
              "      <td>7.222340</td>\n",
              "      <td>337120.849949</td>\n",
              "      <td>4.156004e+06</td>\n",
              "      <td>236.446298</td>\n",
              "      <td>247.332224</td>\n",
              "      <td>181.0</td>\n",
              "      <td>1.890440</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.0</td>\n",
              "      <td>346170.352873</td>\n",
              "      <td>4.149851e+06</td>\n",
              "      <td>-1250.400186</td>\n",
              "      <td>546.793831</td>\n",
              "      <td>9.889338</td>\n",
              "      <td>344369.415740</td>\n",
              "      <td>4.150610e+06</td>\n",
              "      <td>-1800.937133</td>\n",
              "      <td>758.392122</td>\n",
              "      <td>...</td>\n",
              "      <td>247.332224</td>\n",
              "      <td>181.0</td>\n",
              "      <td>1.890440</td>\n",
              "      <td>337114.382547</td>\n",
              "      <td>4.156130e+06</td>\n",
              "      <td>-6.467402</td>\n",
              "      <td>125.446912</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.693997</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.0</td>\n",
              "      <td>344369.415740</td>\n",
              "      <td>4.150610e+06</td>\n",
              "      <td>-1800.937133</td>\n",
              "      <td>758.392122</td>\n",
              "      <td>9.869226</td>\n",
              "      <td>343196.914810</td>\n",
              "      <td>4.151131e+06</td>\n",
              "      <td>-1172.500929</td>\n",
              "      <td>521.606948</td>\n",
              "      <td>...</td>\n",
              "      <td>125.446912</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.693997</td>\n",
              "      <td>336177.869837</td>\n",
              "      <td>4.155345e+06</td>\n",
              "      <td>-936.512710</td>\n",
              "      <td>-784.454321</td>\n",
              "      <td>1202.0</td>\n",
              "      <td>1.016346</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66001</th>\n",
              "      <td>3.0</td>\n",
              "      <td>316873.402193</td>\n",
              "      <td>4.155923e+06</td>\n",
              "      <td>701.842849</td>\n",
              "      <td>-1240.239599</td>\n",
              "      <td>7.422154</td>\n",
              "      <td>317386.113034</td>\n",
              "      <td>4.155027e+06</td>\n",
              "      <td>512.710842</td>\n",
              "      <td>-895.970819</td>\n",
              "      <td>...</td>\n",
              "      <td>-996.591755</td>\n",
              "      <td>131.0</td>\n",
              "      <td>7.610401</td>\n",
              "      <td>319794.487192</td>\n",
              "      <td>4.147384e+06</td>\n",
              "      <td>-62.082231</td>\n",
              "      <td>-992.105461</td>\n",
              "      <td>132.0</td>\n",
              "      <td>7.530652</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66002</th>\n",
              "      <td>3.0</td>\n",
              "      <td>317386.113034</td>\n",
              "      <td>4.155027e+06</td>\n",
              "      <td>512.710842</td>\n",
              "      <td>-895.970819</td>\n",
              "      <td>7.480410</td>\n",
              "      <td>317850.216394</td>\n",
              "      <td>4.154220e+06</td>\n",
              "      <td>464.103359</td>\n",
              "      <td>-807.164377</td>\n",
              "      <td>...</td>\n",
              "      <td>-992.105461</td>\n",
              "      <td>132.0</td>\n",
              "      <td>7.530652</td>\n",
              "      <td>319436.797993</td>\n",
              "      <td>4.146115e+06</td>\n",
              "      <td>-357.689199</td>\n",
              "      <td>-1269.006625</td>\n",
              "      <td>205.0</td>\n",
              "      <td>6.431480</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66003</th>\n",
              "      <td>3.0</td>\n",
              "      <td>317850.216394</td>\n",
              "      <td>4.154220e+06</td>\n",
              "      <td>464.103359</td>\n",
              "      <td>-807.164377</td>\n",
              "      <td>7.569740</td>\n",
              "      <td>318292.944454</td>\n",
              "      <td>4.153442e+06</td>\n",
              "      <td>442.728060</td>\n",
              "      <td>-777.912156</td>\n",
              "      <td>...</td>\n",
              "      <td>-1269.006625</td>\n",
              "      <td>205.0</td>\n",
              "      <td>6.431480</td>\n",
              "      <td>318963.014609</td>\n",
              "      <td>4.145493e+06</td>\n",
              "      <td>-473.783384</td>\n",
              "      <td>-622.380551</td>\n",
              "      <td>131.0</td>\n",
              "      <td>5.970950</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66004</th>\n",
              "      <td>3.0</td>\n",
              "      <td>318292.944454</td>\n",
              "      <td>4.153442e+06</td>\n",
              "      <td>442.728060</td>\n",
              "      <td>-777.912156</td>\n",
              "      <td>7.458941</td>\n",
              "      <td>318738.306698</td>\n",
              "      <td>4.152655e+06</td>\n",
              "      <td>445.362245</td>\n",
              "      <td>-786.821275</td>\n",
              "      <td>...</td>\n",
              "      <td>-622.380551</td>\n",
              "      <td>131.0</td>\n",
              "      <td>5.970950</td>\n",
              "      <td>318389.705915</td>\n",
              "      <td>4.145186e+06</td>\n",
              "      <td>-573.308693</td>\n",
              "      <td>-307.355356</td>\n",
              "      <td>131.0</td>\n",
              "      <td>4.965648</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66005</th>\n",
              "      <td>3.0</td>\n",
              "      <td>318738.306698</td>\n",
              "      <td>4.152655e+06</td>\n",
              "      <td>445.362245</td>\n",
              "      <td>-786.821275</td>\n",
              "      <td>7.534344</td>\n",
              "      <td>319187.487089</td>\n",
              "      <td>4.151877e+06</td>\n",
              "      <td>449.180390</td>\n",
              "      <td>-777.993407</td>\n",
              "      <td>...</td>\n",
              "      <td>-307.355356</td>\n",
              "      <td>131.0</td>\n",
              "      <td>4.965648</td>\n",
              "      <td>318069.840146</td>\n",
              "      <td>4.145537e+06</td>\n",
              "      <td>-319.865769</td>\n",
              "      <td>351.338148</td>\n",
              "      <td>139.0</td>\n",
              "      <td>3.418232</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>57616 rows × 67 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4b3f998-ea21-4a88-b3fc-c95a766fa23d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f4b3f998-ea21-4a88-b3fc-c95a766fa23d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f4b3f998-ea21-4a88-b3fc-c95a766fa23d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster=2\n",
        "grouped=df0.groupby(df0['dataset_tr1_val2_test3_augm_0(t-10)'])\n",
        "df_tr = grouped.get_group(1.0)\n",
        "grouped_cluster=df_tr.groupby(df_tr['cluster'])\n",
        "df_tr = grouped_cluster.get_group(cluster)\n",
        "\n",
        "# Create a function to generate synthetic data\n",
        "def generate_synthetic_data(df, num_rows):\n",
        "    mean = df.mean()\n",
        "    std = df.std()\n",
        "    synthetic_data = []\n",
        "    for i in range(num_rows):\n",
        "        synthetic_row = []\n",
        "        for col_mean, col_std in zip(mean, std):\n",
        "            synthetic_value = np.random.normal(loc=col_mean, scale=(col_std/4))\n",
        "            synthetic_row.append(synthetic_value)\n",
        "        synthetic_data.append(synthetic_row)\n",
        "    return pd.DataFrame(synthetic_data, columns=df.columns)\n",
        "# usage\n",
        "synthetic_data = generate_synthetic_data(df_tr, 5000)\n",
        "df_tr = df_tr.append(synthetic_data, ignore_index=True)\n",
        "\n",
        "df_tr1=df_tr.copy()\n",
        "#to thelw gia to telos gia na upologisw ta lon-lat(t)\n",
        "dftrainzlon=df_tr1[[\"lon(t-1)\"]]\n",
        "dftrainzlat=df_tr1[[\"lat(t-1)\"]]\n",
        "df_trx=df_tr.filter(regex=(\"^dlon|^dlat*|^speed|^dt\"))\n",
        "df_trx=df_trx.drop(labels=['dlon(t)', 'dlat(t)'], axis=1)\n",
        "df_try=df_tr1[[\"dlon(t)\",\"dlat(t)\"]]\n",
        "\n",
        "df_val= grouped.get_group(2.0)\n",
        "grouped_cluster=df_val.groupby(df_val['cluster'])\n",
        "df_val = grouped_cluster.get_group(cluster)\n",
        "df_val1=df_val.copy()\n",
        "#gia to telos \n",
        "dfvalzlon=df_val1[[\"lon(t-1)\"]]\n",
        "dfvalzlat=df_val1[[\"lat(t-1)\"]]\n",
        "\n",
        "df_valx=df_val.filter(regex=(\"^dlon|^dlat*|^speed|^dt\"))\n",
        "df_valx=df_valx.drop(labels=['dlon(t)', 'dlat(t)'], axis=1)\n",
        "df_valy=df_val1[[\"dlon(t)\",\"dlat(t)\"]]\n",
        "\n",
        "df_test = grouped.get_group(3.0)\n",
        "grouped_cluster=df_test.groupby(df_test['cluster'])\n",
        "df_test = grouped_cluster.get_group(cluster)\n",
        "df_test1=df_test.copy()\n",
        "#to thelw gia to telos\n",
        "dftestzlon=df_test1[[\"lon(t-1)\"]]\n",
        "dftestzlat=df_test1[[\"lat(t-1)\"]]\n",
        "\n",
        "df_testx=df_test.filter(regex=(\"^dlon|^dlat*|^speed|^dt\"))\n",
        "df_testx=df_testx.drop(labels=['dlon(t)', 'dlat(t)'], axis=1)\n",
        "df_testy=df_test1[[\"dlon(t)\",\"dlat(t)\"]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "#MinMaxScaler(feature_range=(-1, 1)),StandardScaler()\n",
        "scaler1 = StandardScaler()\n",
        "scaler2 = StandardScaler()\n",
        "scaler3 = StandardScaler()\n",
        "scaler4 = StandardScaler()\n",
        "scaler5 = StandardScaler()\n",
        "scaler6 = StandardScaler()\n",
        "\n",
        "\n",
        "\n",
        "columns=['dlon(t-10)','dlat(t-10)','speed(t-10)','dlon(t-9)','dlat(t-9)','dt(t-9)','speed(t-9)','dlon(t-8)','dlat(t-8)','dt(t-8)','speed(t-8)','dlon(t-7)',\n",
        "         'dlat(t-7)','dt(t-7)','speed(t-7)','dlon(t-6)','dlat(t-6)','dt(t-6)','speed(t-6)','dlon(t-5)','dlat(t-5)','dt(t-5)','speed(t-5)','dlon(t-4)',\n",
        "         'dlat(t-4)','dt(t-4)','speed(t-4)','dlon(t-3)','dlat(t-3)','dt(t-3)','speed(t-3)','dlon(t-2)','dlat(t-2)','dt(t-2)','speed(t-2)','dlon(t-1)',\n",
        "         'dlat(t-1)','dt(t-1)','speed(t-1)','dt(t)','speed(t)']\n",
        "\n",
        "df_trx[columns]=scaler1.fit_transform(df_trx[columns])\n",
        "df_try[['dlon(t)','dlat(t)']]=scaler2.fit_transform(df_try[['dlon(t)','dlat(t)']])\n",
        "df_valx[columns]=scaler3.fit_transform(df_valx[columns])\n",
        "df_valy[['dlon(t)','dlat(t)']]=scaler4.fit_transform(df_valy[['dlon(t)','dlat(t)']])\n",
        "df_testx[columns]=scaler5.fit_transform(df_testx[columns])\n",
        "df_testy[['dlon(t)','dlat(t)']]=scaler6.fit_transform(df_testy[['dlon(t)','dlat(t)']])\n",
        "\n"
      ],
      "metadata": {
        "id": "Ad4lf4v-SIB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer1=\"relu\"\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "    # Building a model with SimpleRNN \n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "#from keras.activations import gaussian\n",
        "from tensorflow.keras.layers import (BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense)\n",
        "#lets try Dropout BatchNormalization shuflle  relu elu esu \n",
        "#define model and model parameters\n",
        "batch_size = 32\n",
        "model = Sequential()\n",
        "n_cols=df_trx.shape[1]\n",
        "n_rows=df_trx.shape[0]\n",
        "epochs=1000\n",
        "patience=10\n",
        "\n",
        "model.add(Dense(units=32, input_shape=(n_cols, ), activation=layer1))#relu,selu,sigmoid,tanh\n",
        "model.add(Dense(16, activation=layer1))\n",
        "model.add(Dense(8, activation=layer1))\n",
        "model.add(Dense(4, activation=layer1))\n",
        "model.add(Dense(2, activation=layer1))\n",
        "\n",
        "#compile model\n",
        "model.compile(loss='mae', optimizer='adam',metrics=['mean_absolute_error']) \n",
        "#fit model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpointer = ModelCheckpoint(filepath='weights.hdf5', save_best_only=True, \n",
        "                               save_weights_only=True, monitor='val_mean_absolute_error', mode='min', verbose=1)\n",
        "\n",
        "early_stopping_monitor = EarlyStopping(patience=patience)\n",
        "history=model.fit(df_trx, df_try, validation_data=(df_testx,df_testy), epochs=epochs, \n",
        "                  callbacks=[early_stopping_monitor,checkpointer], batch_size=batch_size, verbose=1,shuffle=True)\n",
        "model.summary()\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "#metrics to use accuracy, means_squared_error\n",
        "\n",
        "#plt.plot(history.history['mean_squared_error'])\n",
        "#plt.plot(history.history['val_mean_squared_error'])\n",
        "#plt.title('model mean_squared_error')\n",
        "#plt.ylabel('accuracy(mean_squared_error)')\n",
        "#plt.xlabel('epoch')\n",
        "#plt.legend(['train', 'validation'], loc='upper left')\n",
        "#plt.show()\n",
        "# summarize history for loss\n",
        "#plt.plot(history.history['loss'])\n",
        "#plt.plot(history.history['val_loss'])\n",
        "#plt.title('model loss')\n",
        "#plt.ylabel('loss')\n",
        "#plt.xlabel('epoch')\n",
        "#plt.legend(['train', 'validation'], loc='upper left')\n",
        "#plt.show()\n",
        "\n",
        "model.load_weights('weights.hdf5')\n",
        "trainPredict = model.predict(df_trx, batch_size=batch_size)\n",
        "testPredict = model.predict(df_valx, batch_size=batch_size)\n",
        "\n",
        "# invert predictions\n",
        "trainPredict = np.reshape(trainPredict, (-1, 2))\n",
        "ytr2d = np.reshape(df_try, (-1, 2))\n",
        "\n",
        "testPredict = np.reshape(testPredict, (-1, 2))\n",
        "yte2d = np.reshape(df_valy, (-1, 2))\n",
        "trainPredict\n",
        "\n",
        "\n",
        "\n",
        "trainPredict = scaler2.inverse_transform(trainPredict)\n",
        "trainY = scaler2.inverse_transform(ytr2d)\n",
        "\n",
        "testPredict = scaler6.inverse_transform(testPredict)\n",
        "testY = scaler6.inverse_transform(yte2d)\n",
        "\n",
        "dfpredict = pd.DataFrame(testPredict,columns=['dlon','dlat'])\n",
        "dftest= pd.DataFrame(testY,columns=['dlon','dlat'])\n",
        "trainPred= pd.DataFrame(trainPredict,columns=['dlon','dlat'])\n",
        "trainReal= pd.DataFrame(trainY,columns=['dlon','dlat'])\n",
        "\n",
        "\n",
        "\n",
        "dfpredictdlon= pd.DataFrame([])\n",
        "dftestdlon= pd.DataFrame([])\n",
        "trainPreddlon =pd.DataFrame([])\n",
        "trainRealdlon = pd.DataFrame([])\n",
        "dfpredictdlat= pd.DataFrame([])\n",
        "dftestdlat= pd.DataFrame([])\n",
        "trainPreddlat = pd.DataFrame([])\n",
        "trainRealdlat =pd.DataFrame([])\n",
        "\n",
        "dfpredictdlon['dlon']=dfpredict['dlon']\n",
        "dftestdlon['dlon']= dftest['dlon']\n",
        "trainPreddlon['dlon'] = trainPred['dlon']\n",
        "trainRealdlon['dlon'] = trainReal['dlon']\n",
        "dfpredictdlat['dlat']= dfpredict['dlat']\n",
        "dftestdlat['dlat']= dftest['dlat']\n",
        "trainPreddlat['dlat'] = trainPred['dlat']\n",
        "trainRealdlat['dlat'] = trainReal['dlat']\n",
        "\n",
        "\n",
        "dfpredictdlon['lon']=dfvalzlon[\"lon(t-1)\"].values\n",
        "dfpredictdlat['lat'] =dfvalzlat[\"lat(t-1)\"].values\n",
        "dftestdlon['lon']=dfvalzlon[\"lon(t-1)\"].values\n",
        "dftestdlat['lat'] =dfvalzlat[\"lat(t-1)\"].values\n",
        "\n",
        "trainPreddlon['lon'] =dftrainzlon[\"lon(t-1)\"].values\n",
        "trainRealdlon['lon'] =dftrainzlon[\"lon(t-1)\"].values\n",
        "trainPreddlat['lat'] =dftrainzlat[\"lat(t-1)\"].values\n",
        "trainRealdlat['lat'] =dftrainzlat[\"lat(t-1)\"].values\n",
        "\n",
        "\n",
        "\n",
        "#df.sum(axis = 1, skipna = True)\n",
        "\n",
        "trainPredlon= pd.DataFrame([])\n",
        "trainPredlat= pd.DataFrame([])\n",
        "trainReallon= pd.DataFrame([])\n",
        "trainReallat= pd.DataFrame([])\n",
        "dfpredictlon= pd.DataFrame([])\n",
        "dfpredictlat= pd.DataFrame([])\n",
        "dftestlon= pd.DataFrame([])\n",
        "dftestlat= pd.DataFrame([])\n",
        "dfpredictdlon['flon']=dfpredictdlon.sum(axis = 1, skipna = True)\n",
        "dfpredictdlat['flat']= dfpredictdlat.sum(axis = 1, skipna = True)\n",
        "dftestdlon['flon']= dftestdlon.sum(axis = 1, skipna = True)\n",
        "dftestdlat['flat']= dftestdlat.sum(axis = 1, skipna = True)\n",
        "\n",
        "trainPreddlon['flon']= trainPreddlon.sum(axis = 1, skipna = True)\n",
        "trainPreddlat['flat']= trainPreddlat.sum(axis = 1, skipna = True)\n",
        "trainRealdlon['flon']= trainRealdlon.sum(axis = 1, skipna = True)\n",
        "trainRealdlat['flat']= trainRealdlat.sum(axis = 1, skipna = True)\n",
        "\n",
        "trainPredlon['lon']= trainPreddlon['flon']\n",
        "trainPredlat['lat']= trainPreddlat['flat']\n",
        "trainReallon['lon']= trainRealdlon['flon']\n",
        "trainReallat['lat']= trainRealdlat['flat']\n",
        "dfpredictlon['lon']=dfpredictdlon['flon']\n",
        "dfpredictlat['lat']=dfpredictdlat['flat']\n",
        "dftestlon['lon']=dftestdlon['flon']\n",
        "dftestlat['lat']=dftestdlat['flat']\n",
        "\n",
        "print(\"Test northing MSE: \", mean_squared_error(dftestlat, dfpredictlat))\n",
        "\n",
        "print(\"Test northing R2: \", r2_score(dftestlon, dfpredictlon))\n",
        "\n",
        "print(\"Test easting MSE: \", mean_squared_error(dftestlon, dfpredictlon))\n",
        "\n",
        "print(\"Test easting R2: \", r2_score(dftestlat, dfpredictlat))\n",
        "\n",
        "print(\"Test northing MAE: \", mean_absolute_error(dftestlat, dfpredictlat))\n",
        "\n",
        "print(\"Test easting MAE: \", mean_absolute_error(dftestlon, dfpredictlon))\n",
        "\n",
        "df_predtrainresults= pd.DataFrame([])\n",
        "df_trainresults = pd.DataFrame([])\n",
        "df1= pd.DataFrame([])\n",
        "\n",
        "df_predtrainresults[\"long_latPREDICT\"] = list(zip(trainPredlon['lon'], trainPredlat['lat']))\n",
        "df_trainresults[\"long_latRESULT\"] = list(zip(trainReallon['lon'], trainReallat['lat']))\n",
        "\n",
        "df1['prediction']=df_predtrainresults[\"long_latPREDICT\"]\n",
        "df1['reality']=df_trainresults[\"long_latRESULT\"]\n",
        "\n",
        "def distance(df1):\n",
        "  return math.dist(df1['prediction'],df1['reality'])\n",
        "\n",
        "df1[\"distance(m)\"] = df1.apply(distance, axis =1)\n",
        "df1[\"distance(m)\"].mean()\n",
        "\n",
        "df_predresults = pd.DataFrame([])\n",
        "df_testresults = pd.DataFrame([])\n",
        "df= pd.DataFrame([])\n",
        "df_predresults[\"long_latPREDICT\"] = list(zip(dfpredictlon['lon'], dfpredictlat['lat']))\n",
        "df_testresults[\"long_latRESULT\"] = list(zip(dftestlon['lon'], dftestlat['lat']))\n",
        "\n",
        "df['prediction']=df_predresults[\"long_latPREDICT\"]\n",
        "df['reality']=df_testresults[\"long_latRESULT\"]\n",
        "\n",
        "\n",
        "#def distance(df):\n",
        "#    return math.dist(df['prediction'],df['reality'])\n",
        "\n",
        "#df[\"distance(m)\"] = df.apply(distance, axis =1)\n",
        "#df[\"distance(m)\"].mean()\n",
        "\n",
        "testxdf = scaler5.inverse_transform(df_testx)\n",
        "dfftestx = pd.DataFrame(testxdf,columns=columns)\n",
        "\n",
        "df[\"dt\"]=dfftestx['dt(t)']\n",
        "#max(df['distance(m)'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSiAcgG3-FM1",
        "outputId": "cf5f48ae-5eb0-4491-b362-ec1566a6038f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "414/421 [============================>.] - ETA: 0s - loss: 0.5739 - mean_absolute_error: 0.5739\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 0.57353, saving model to weights.hdf5\n",
            "421/421 [==============================] - 3s 3ms/step - loss: 0.5736 - mean_absolute_error: 0.5736 - val_loss: 0.5735 - val_mean_absolute_error: 0.5735\n",
            "Epoch 2/1000\n",
            "397/421 [===========================>..] - ETA: 0s - loss: 0.4997 - mean_absolute_error: 0.4997\n",
            "Epoch 2: val_mean_absolute_error improved from 0.57353 to 0.55563, saving model to weights.hdf5\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4986 - mean_absolute_error: 0.4986 - val_loss: 0.5556 - val_mean_absolute_error: 0.5556\n",
            "Epoch 3/1000\n",
            "415/421 [============================>.] - ETA: 0s - loss: 0.4818 - mean_absolute_error: 0.4818\n",
            "Epoch 3: val_mean_absolute_error improved from 0.55563 to 0.52931, saving model to weights.hdf5\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4820 - mean_absolute_error: 0.4820 - val_loss: 0.5293 - val_mean_absolute_error: 0.5293\n",
            "Epoch 4/1000\n",
            "392/421 [==========================>...] - ETA: 0s - loss: 0.4724 - mean_absolute_error: 0.4724\n",
            "Epoch 4: val_mean_absolute_error improved from 0.52931 to 0.51781, saving model to weights.hdf5\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4726 - mean_absolute_error: 0.4726 - val_loss: 0.5178 - val_mean_absolute_error: 0.5178\n",
            "Epoch 5/1000\n",
            "392/421 [==========================>...] - ETA: 0s - loss: 0.4650 - mean_absolute_error: 0.4650\n",
            "Epoch 5: val_mean_absolute_error improved from 0.51781 to 0.51009, saving model to weights.hdf5\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4658 - mean_absolute_error: 0.4658 - val_loss: 0.5101 - val_mean_absolute_error: 0.5101\n",
            "Epoch 6/1000\n",
            "415/421 [============================>.] - ETA: 0s - loss: 0.4618 - mean_absolute_error: 0.4618\n",
            "Epoch 6: val_mean_absolute_error did not improve from 0.51009\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4622 - mean_absolute_error: 0.4622 - val_loss: 0.5116 - val_mean_absolute_error: 0.5116\n",
            "Epoch 7/1000\n",
            "416/421 [============================>.] - ETA: 0s - loss: 0.4583 - mean_absolute_error: 0.4583\n",
            "Epoch 7: val_mean_absolute_error improved from 0.51009 to 0.50668, saving model to weights.hdf5\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4577 - mean_absolute_error: 0.4577 - val_loss: 0.5067 - val_mean_absolute_error: 0.5067\n",
            "Epoch 8/1000\n",
            "415/421 [============================>.] - ETA: 0s - loss: 0.4539 - mean_absolute_error: 0.4539\n",
            "Epoch 8: val_mean_absolute_error improved from 0.50668 to 0.50002, saving model to weights.hdf5\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4537 - mean_absolute_error: 0.4537 - val_loss: 0.5000 - val_mean_absolute_error: 0.5000\n",
            "Epoch 9/1000\n",
            "418/421 [============================>.] - ETA: 0s - loss: 0.4510 - mean_absolute_error: 0.4510\n",
            "Epoch 9: val_mean_absolute_error did not improve from 0.50002\n",
            "421/421 [==============================] - 1s 3ms/step - loss: 0.4512 - mean_absolute_error: 0.4512 - val_loss: 0.5013 - val_mean_absolute_error: 0.5013\n",
            "Epoch 10/1000\n",
            "398/421 [===========================>..] - ETA: 0s - loss: 0.4508 - mean_absolute_error: 0.4508\n",
            "Epoch 10: val_mean_absolute_error did not improve from 0.50002\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4497 - mean_absolute_error: 0.4497 - val_loss: 0.5039 - val_mean_absolute_error: 0.5039\n",
            "Epoch 11/1000\n",
            "395/421 [===========================>..] - ETA: 0s - loss: 0.4465 - mean_absolute_error: 0.4465\n",
            "Epoch 11: val_mean_absolute_error did not improve from 0.50002\n",
            "421/421 [==============================] - 1s 3ms/step - loss: 0.4471 - mean_absolute_error: 0.4471 - val_loss: 0.5028 - val_mean_absolute_error: 0.5028\n",
            "Epoch 12/1000\n",
            "393/421 [===========================>..] - ETA: 0s - loss: 0.4451 - mean_absolute_error: 0.4451\n",
            "Epoch 12: val_mean_absolute_error did not improve from 0.50002\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4451 - mean_absolute_error: 0.4451 - val_loss: 0.5060 - val_mean_absolute_error: 0.5060\n",
            "Epoch 13/1000\n",
            "416/421 [============================>.] - ETA: 0s - loss: 0.4440 - mean_absolute_error: 0.4440\n",
            "Epoch 13: val_mean_absolute_error did not improve from 0.50002\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4438 - mean_absolute_error: 0.4438 - val_loss: 0.5042 - val_mean_absolute_error: 0.5042\n",
            "Epoch 14/1000\n",
            "414/421 [============================>.] - ETA: 0s - loss: 0.4437 - mean_absolute_error: 0.4437\n",
            "Epoch 14: val_mean_absolute_error improved from 0.50002 to 0.49676, saving model to weights.hdf5\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4428 - mean_absolute_error: 0.4428 - val_loss: 0.4968 - val_mean_absolute_error: 0.4968\n",
            "Epoch 15/1000\n",
            "418/421 [============================>.] - ETA: 0s - loss: 0.4408 - mean_absolute_error: 0.4408\n",
            "Epoch 15: val_mean_absolute_error improved from 0.49676 to 0.49653, saving model to weights.hdf5\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4412 - mean_absolute_error: 0.4412 - val_loss: 0.4965 - val_mean_absolute_error: 0.4965\n",
            "Epoch 16/1000\n",
            "403/421 [===========================>..] - ETA: 0s - loss: 0.4406 - mean_absolute_error: 0.4406\n",
            "Epoch 16: val_mean_absolute_error did not improve from 0.49653\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4407 - mean_absolute_error: 0.4407 - val_loss: 0.5030 - val_mean_absolute_error: 0.5030\n",
            "Epoch 17/1000\n",
            "416/421 [============================>.] - ETA: 0s - loss: 0.4399 - mean_absolute_error: 0.4399\n",
            "Epoch 17: val_mean_absolute_error did not improve from 0.49653\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4399 - mean_absolute_error: 0.4399 - val_loss: 0.5082 - val_mean_absolute_error: 0.5082\n",
            "Epoch 18/1000\n",
            "406/421 [===========================>..] - ETA: 0s - loss: 0.4399 - mean_absolute_error: 0.4399\n",
            "Epoch 18: val_mean_absolute_error improved from 0.49653 to 0.49546, saving model to weights.hdf5\n",
            "421/421 [==============================] - 1s 3ms/step - loss: 0.4384 - mean_absolute_error: 0.4384 - val_loss: 0.4955 - val_mean_absolute_error: 0.4955\n",
            "Epoch 19/1000\n",
            "410/421 [============================>.] - ETA: 0s - loss: 0.4387 - mean_absolute_error: 0.4387\n",
            "Epoch 19: val_mean_absolute_error did not improve from 0.49546\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4385 - mean_absolute_error: 0.4385 - val_loss: 0.4979 - val_mean_absolute_error: 0.4979\n",
            "Epoch 20/1000\n",
            "399/421 [===========================>..] - ETA: 0s - loss: 0.4369 - mean_absolute_error: 0.4369\n",
            "Epoch 20: val_mean_absolute_error improved from 0.49546 to 0.49025, saving model to weights.hdf5\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4377 - mean_absolute_error: 0.4377 - val_loss: 0.4902 - val_mean_absolute_error: 0.4902\n",
            "Epoch 21/1000\n",
            "394/421 [===========================>..] - ETA: 0s - loss: 0.4364 - mean_absolute_error: 0.4364\n",
            "Epoch 21: val_mean_absolute_error did not improve from 0.49025\n",
            "421/421 [==============================] - 1s 3ms/step - loss: 0.4367 - mean_absolute_error: 0.4367 - val_loss: 0.5051 - val_mean_absolute_error: 0.5051\n",
            "Epoch 22/1000\n",
            "413/421 [============================>.] - ETA: 0s - loss: 0.4375 - mean_absolute_error: 0.4375\n",
            "Epoch 22: val_mean_absolute_error did not improve from 0.49025\n",
            "421/421 [==============================] - 2s 4ms/step - loss: 0.4371 - mean_absolute_error: 0.4371 - val_loss: 0.4913 - val_mean_absolute_error: 0.4913\n",
            "Epoch 23/1000\n",
            "420/421 [============================>.] - ETA: 0s - loss: 0.4358 - mean_absolute_error: 0.4358\n",
            "Epoch 23: val_mean_absolute_error did not improve from 0.49025\n",
            "421/421 [==============================] - 2s 5ms/step - loss: 0.4363 - mean_absolute_error: 0.4363 - val_loss: 0.4934 - val_mean_absolute_error: 0.4934\n",
            "Epoch 24/1000\n",
            "403/421 [===========================>..] - ETA: 0s - loss: 0.4356 - mean_absolute_error: 0.4356\n",
            "Epoch 24: val_mean_absolute_error improved from 0.49025 to 0.48905, saving model to weights.hdf5\n",
            "421/421 [==============================] - 1s 3ms/step - loss: 0.4357 - mean_absolute_error: 0.4357 - val_loss: 0.4891 - val_mean_absolute_error: 0.4891\n",
            "Epoch 25/1000\n",
            "394/421 [===========================>..] - ETA: 0s - loss: 0.4340 - mean_absolute_error: 0.4340\n",
            "Epoch 25: val_mean_absolute_error did not improve from 0.48905\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4349 - mean_absolute_error: 0.4349 - val_loss: 0.5042 - val_mean_absolute_error: 0.5042\n",
            "Epoch 26/1000\n",
            "393/421 [===========================>..] - ETA: 0s - loss: 0.4342 - mean_absolute_error: 0.4342\n",
            "Epoch 26: val_mean_absolute_error did not improve from 0.48905\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4347 - mean_absolute_error: 0.4347 - val_loss: 0.4967 - val_mean_absolute_error: 0.4967\n",
            "Epoch 27/1000\n",
            "391/421 [==========================>...] - ETA: 0s - loss: 0.4345 - mean_absolute_error: 0.4345\n",
            "Epoch 27: val_mean_absolute_error did not improve from 0.48905\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4343 - mean_absolute_error: 0.4343 - val_loss: 0.4959 - val_mean_absolute_error: 0.4959\n",
            "Epoch 28/1000\n",
            "392/421 [==========================>...] - ETA: 0s - loss: 0.4340 - mean_absolute_error: 0.4340\n",
            "Epoch 28: val_mean_absolute_error did not improve from 0.48905\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4345 - mean_absolute_error: 0.4345 - val_loss: 0.5024 - val_mean_absolute_error: 0.5024\n",
            "Epoch 29/1000\n",
            "421/421 [==============================] - ETA: 0s - loss: 0.4345 - mean_absolute_error: 0.4345\n",
            "Epoch 29: val_mean_absolute_error did not improve from 0.48905\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4345 - mean_absolute_error: 0.4345 - val_loss: 0.4962 - val_mean_absolute_error: 0.4962\n",
            "Epoch 30/1000\n",
            "393/421 [===========================>..] - ETA: 0s - loss: 0.4328 - mean_absolute_error: 0.4328\n",
            "Epoch 30: val_mean_absolute_error did not improve from 0.48905\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4334 - mean_absolute_error: 0.4334 - val_loss: 0.4975 - val_mean_absolute_error: 0.4975\n",
            "Epoch 31/1000\n",
            "399/421 [===========================>..] - ETA: 0s - loss: 0.4345 - mean_absolute_error: 0.4345\n",
            "Epoch 31: val_mean_absolute_error did not improve from 0.48905\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4334 - mean_absolute_error: 0.4334 - val_loss: 0.4975 - val_mean_absolute_error: 0.4975\n",
            "Epoch 32/1000\n",
            "394/421 [===========================>..] - ETA: 0s - loss: 0.4334 - mean_absolute_error: 0.4334\n",
            "Epoch 32: val_mean_absolute_error did not improve from 0.48905\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4332 - mean_absolute_error: 0.4332 - val_loss: 0.4907 - val_mean_absolute_error: 0.4907\n",
            "Epoch 33/1000\n",
            "390/421 [==========================>...] - ETA: 0s - loss: 0.4331 - mean_absolute_error: 0.4331\n",
            "Epoch 33: val_mean_absolute_error did not improve from 0.48905\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4336 - mean_absolute_error: 0.4336 - val_loss: 0.4958 - val_mean_absolute_error: 0.4958\n",
            "Epoch 34/1000\n",
            "418/421 [============================>.] - ETA: 0s - loss: 0.4334 - mean_absolute_error: 0.4334\n",
            "Epoch 34: val_mean_absolute_error did not improve from 0.48905\n",
            "421/421 [==============================] - 1s 2ms/step - loss: 0.4331 - mean_absolute_error: 0.4331 - val_loss: 0.4913 - val_mean_absolute_error: 0.4913\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_95 (Dense)            (None, 32)                1344      \n",
            "                                                                 \n",
            " dense_96 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dense_97 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_98 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " dense_99 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,054\n",
            "Trainable params: 2,054\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "dict_keys(['loss', 'mean_absolute_error', 'val_loss', 'val_mean_absolute_error'])\n",
            "421/421 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "Test northing MSE:  730165.1599517126\n",
            "Test northing R2:  0.9991753888343264\n",
            "Test easting MSE:  812653.077112599\n",
            "Test easting R2:  0.9990375358597416\n",
            "Test northing MAE:  619.8442173602594\n",
            "Test easting MAE:  673.0427586439214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "index1=0\n",
        "index2=0\n",
        "index3=0\n",
        "index4=0\n",
        "index5=0\n",
        "index6=0\n",
        "index7=0\n",
        "index8=0\n",
        "index9=0\n",
        "index10=0\n",
        "index11=0\n",
        "index12=0\n",
        "index13=0\n",
        "index14=0\n",
        "index15=0\n",
        "index16=0\n",
        "index17=0\n",
        "index18=0\n",
        "index19=0\n",
        "index20=0\n",
        "index21=0\n",
        "index22=0\n",
        "index23=0\n",
        "index24=0\n",
        "index25=0\n",
        "index26=0\n",
        "index27=0\n",
        "index28=0\n",
        "index29=0\n",
        "dist1740=0\n",
        "dist1680=0\n",
        "dist1620=0\n",
        "dist1560=0\n",
        "dist1500=0\n",
        "dist1440=0\n",
        "dist1380=0\n",
        "dist1320=0\n",
        "dist1260=0\n",
        "dist1200=0\n",
        "dist1140=0\n",
        "dist1080=0\n",
        "dist1020=0\n",
        "dist960=0\n",
        "dist900=0\n",
        "dist840=0\n",
        "dist780=0\n",
        "dist720=0\n",
        "dist660=0\n",
        "dist600=0\n",
        "dist540=0\n",
        "dist480=0\n",
        "dist420=0\n",
        "dist360=0\n",
        "dist300=0\n",
        "dist240=0\n",
        "dist180=0\n",
        "dist120=0\n",
        "dist60=0\n",
        "for index, row in df.iterrows(): \n",
        "    window= row[2]\n",
        "    if window>=1740 and window<=1800:\n",
        "            dist1740=dist1740+row[2] \n",
        "            index1=index1+1\n",
        "    if window>=1680 and window<1740:\n",
        "            dist1680=dist1680+row[2] \n",
        "            index2=index2+1\n",
        "    if window>=1620 and window<1680:\n",
        "            dist1620=dist1620+row[2] \n",
        "            index3=index3+1\n",
        "    if window>=1560 and window<1620:\n",
        "            dist1560=dist1560+row[2] \n",
        "            index4=index4+1\n",
        "    if window>=1500 and window<1560:\n",
        "            dist1500=dist1500+row[2] \n",
        "            index5=index5+1\n",
        "    if window>=1440 and window<1500:\n",
        "            dist1440=dist1440+row[2] \n",
        "            index6=index6+1\n",
        "    if window>=1380 and window<1440:\n",
        "            dist1380=dist1380+row[2] \n",
        "            index7=index7+1\n",
        "    if window>=1320 and window<1380:\n",
        "            dist1320=dist1320+row[2] \n",
        "            index8=index8+1\n",
        "    if window>=1260 and window<1320:\n",
        "            dist1260=dist1260+row[2] \n",
        "            index9=index9+1    \n",
        "    if window>=1200 and window<1260:\n",
        "            dist1200=dist1200+row[2] \n",
        "            index10=index10+1\n",
        "    if window>=1140 and window<1200:\n",
        "            dist1140=dist1140+row[2] \n",
        "            index11=index11+1\n",
        "    if window>=1080 and window<1140:\n",
        "            dist1080=dist1080+row[2] \n",
        "            index12=index12+1\n",
        "    if window>=1020 and window<1080:\n",
        "            dist1020=dist1020+row[2] \n",
        "            index13=index13+1\n",
        "    if window>=960 and window<1020:\n",
        "            dist960=dist960+row[2] \n",
        "            index14=index14+1\n",
        "    if window>=900 and window<960:\n",
        "            dist900=dist900+row[2] \n",
        "            index15=index15+1\n",
        "    if window>=840 and window<900:\n",
        "            dist840=dist840+row[2] \n",
        "            index16=index16+1\n",
        "    if window>=780 and window<840:\n",
        "            dist780=dist780+row[2] \n",
        "            index17=index17+1\n",
        "    if window>=720 and window<780:\n",
        "            dist720=dist720+row[2] \n",
        "            index18=index18+1\n",
        "    if window>=660 and window<720:\n",
        "            dist660=dist660+row[2] \n",
        "            index19=index19+1\n",
        "    if window>=600 and window<660:\n",
        "            dist600=dist600+row[2] \n",
        "            index20=index20+1\n",
        "    if window>=540 and window<600:\n",
        "            dist540=dist540+row[2] \n",
        "            index21=index21+1\n",
        "    if window>=480 and window<540:\n",
        "            dist480=dist480+row[2] \n",
        "            index22=index22+1\n",
        "    if window>=420 and window<480:\n",
        "            dist420=dist420+row[2] \n",
        "            index23=index23+1\n",
        "    if window>=360 and window<420:\n",
        "            dist360=dist360+row[2] \n",
        "            index24=index24+1\n",
        "    if window>=300 and window<360:\n",
        "            dist300=dist300+row[2] \n",
        "            index25=index25+1\n",
        "    if window>=240 and window<300:\n",
        "            dist240=dist240+row[2] \n",
        "            index26=index26+1\n",
        "    if window>=180 and window<240:\n",
        "            dist180=dist180+row[2] \n",
        "            index27=index27+1\n",
        "    if window>=120 and window<180:\n",
        "            dist120=dist120+row[2] \n",
        "            index28=index28+1\n",
        "    if window>=0 and window<120:\n",
        "            dist60=dist60+row[2] \n",
        "            index29=index29+1\n",
        "\n",
        "\n",
        "print( '[0,2)&' ,(dist60/index29)/1000, index29)\n",
        "print( '[2,3)&' ,(dist120/index28)/1000,index28)\n",
        "print( '[3,4)&' ,(dist180/index27)/1000,index27)  \n",
        "print( '[4,5)&' ,(dist240/index26)/1000,index26)  \n",
        "print( '[5,6)&' ,(dist300/index25) /1000,index25) \n",
        "print( '[6,7)&' ,(dist360/index24) /1000,index24) \n",
        "print( '[7,8)&' ,(dist420/index23)/1000,index23)\n",
        "print( '[8,9)&' ,(dist480/index22) /1000,index22) \n",
        "print( '[9,10)&' ,(dist540/index21) /1000,index21) \n",
        "print( '[10,11)&' ,(dist600/index20)/1000,index20) \n",
        "print( '[11,12)&' ,(dist660/index19) /1000,index19) \n",
        "print( '[12,13)&' ,(dist720/index18)/1000,index18)\n",
        "print( '[13,14)&' ,(dist780 /index17) /1000,index17) \n",
        "print( '[14,15)&' ,(dist840/index16) /1000,index16) \n",
        "print( '[15,16)&' ,(dist900/index15)/1000,index15) \n",
        "print( '[16,17)&' ,(dist960/index14) /1000,index14) \n",
        "print( '[17,18)&' ,(dist1020/index13)/1000,index13)\n",
        "print( '[18,19)&' ,(dist1080/index12) /1000,index12) \n",
        "print( '[19,20)&' ,(dist1140/index11) /1000,index11) \n",
        "print( '[20,21)&' ,(dist1200/index10)/1000,index10) \n",
        "print( '[21,22)&' ,(dist1260/index9)  /1000,index9)\n",
        "print( '[22,23)&' ,(dist1320 /index8)/1000,index8)\n",
        "print( '[23,24)&' ,(dist1380/index7) /1000,index7) \n",
        "print( '[24,25)&' ,(dist1440/index6)/1000,index6)  \n",
        "print( '[25,26)&' ,(dist1500/index5)/1000,index5) \n",
        "print( '[26,27)&' ,(dist1560/index4)/1000,index4)  \n",
        "print( '[27,28)&' ,(dist1620/index3)/1000,index3)\n",
        "print( '[28,29)&' ,(dist1680/index2)/1000,index2)  \n",
        "print( '[29,30)&' ,(dist1740/index1)/1000,index1)  \n",
        "\n"
      ],
      "metadata": {
        "id": "HSMHJmzpcnNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43698ef-57e5-42bc-9a43-bbd646363760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0,2)& 0.10947619047619048 42\n",
            "[2,3)& 0.13472530953066514 3473\n",
            "[3,4)& 0.19470916334661353 1255\n",
            "[4,5)& 0.2631311475409836 61\n",
            "[5,6)& 0.323 25\n",
            "[6,7)& 0.38535714285714284 14\n",
            "[7,8)& 0.4410909090909091 11\n",
            "[8,9)& 0.5064615384615384 13\n",
            "[9,10)& 0.5658 15\n",
            "[10,11)& 0.6212000000000001 5\n",
            "[11,12)& 0.6905333333333333 15\n",
            "[12,13)& 0.746875 8\n",
            "[13,14)& 0.804 6\n",
            "[14,15)& 0.8666666666666666 12\n",
            "[15,16)& 0.9295 2\n",
            "[16,17)& 0.9773999999999999 5\n",
            "[17,18)& 1.0438333333333332 6\n",
            "[18,19)& 1.1083333333333332 3\n",
            "[19,20)& 1.1775 2\n",
            "[20,21)& 1.228 5\n",
            "[21,22)& 1.2725 2\n",
            "[22,23)& 1.35025 8\n",
            "[23,24)& 1.408 3\n",
            "[24,25)& 1.469 2\n",
            "[25,26)& 1.53375 4\n",
            "[26,27)& 1.5823333333333331 3\n",
            "[27,28)& 1.653 1\n",
            "[28,29)& 1.7146666666666668 3\n",
            "[29,30)& 1.7575 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Double Clustered Data Models "
      ],
      "metadata": {
        "id": "I2-x-zqj6BfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = pd.read_pickle('/content/drive/MyDrive/t10df_20210719.pkl')\n",
        "\n",
        "dfall = pd.read_csv('/content/drive/MyDrive/dfdist66037minlns4573.csv',index_col=0)\n",
        "grouped=dfall.groupby(dfall['clusterId'])\n",
        "df_0 = grouped.get_group(0).copy()\n",
        "df_1 = grouped.get_group(1).copy()\n",
        "df_2 = grouped.get_group(2).copy()\n",
        "tags_of_0=df_0['tags'].unique()\n",
        "tags_of_1=df_1['tags'].unique()\n",
        "tags_of_2=df_2['tags'].unique()\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_pickle('/content/drive/MyDrive/t10df_20210719.pkl')\n",
        "\n",
        "dfall = pd.read_csv('/content/drive/MyDrive/dfdist66037minlns4573.csv',index_col=0)\n",
        "grouped=dfall.groupby(dfall['clusterId'])\n",
        "df_0 = grouped.get_group(0).copy()\n",
        "df_1 = grouped.get_group(1).copy()\n",
        "df_2 = grouped.get_group(2).copy()\n",
        "tags_of_0=df_0['tags'].unique()\n",
        "tags_of_1=df_1['tags'].unique()\n",
        "tags_of_2=df_2['tags'].unique()\n",
        "\n",
        "df=data.filter(regex=(\"^lon|^lat*\"))\n",
        "df['rtr0diff']=np.nan\n",
        "df['rtr1diff']=np.nan\n",
        "df['rtr2diff']=np.nan\n",
        "df=df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  line=np.array([])\n",
        "  diff0=0\n",
        "  diff1=0\n",
        "  diff2=0\n",
        "  line=df.loc[i]\n",
        "\n",
        "  for j in range(1,22,2):\n",
        "      diff0= diff0+abs(rtr0_f(line[j-1])-line[j])\n",
        "      diff1= diff1+abs(rtr1_f(line[j-1])-line[j])\n",
        "      diff2= diff2+abs(rtr2_f(line[j-1])-line[j])\n",
        "\n",
        "  df.rtr0diff[i]=diff0\n",
        "  df.rtr1diff[i]=diff1\n",
        "  df.rtr2diff[i]=diff2\n",
        "\n",
        "\n",
        "df_selected = df.loc[:,['rtr0diff','rtr1diff','rtr2diff']]\n",
        "min_col = np.argmin(df_selected.values,axis=1)\n",
        "df['cluster']=min_col\n",
        "data['cluster']=min_col\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import glob\n",
        "import shutil\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "from pandas import concat\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#pip install -U scikit-learn scipy matplotlib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data['column_mmsi'] = np.where(data[\"mmsi(t-1)\"] == data[\"mmsi(t)\"], True, False)\n",
        "false_count = (~data.column_mmsi).sum()\n",
        "true_count = (data.column_mmsi).sum()\n",
        "print(\"num of false mmsi\",(false_count))\n",
        "print(\"num of true mmsi\",(true_count))\n",
        "\n",
        "data['column_id'] = np.where(data[\"id(t-1)\"] == data[\"id(t)\"], True, False)\n",
        "false_count_id = (~data.column_id).sum()\n",
        "true_count_id = (data.column_id).sum()\n",
        "print(\"num of false id\",(false_count_id))\n",
        "print(\"num of true id\",(true_count_id))\n",
        "\n",
        "#to id dinei to trip (mporw na xrhsimopoihsw ws timegap 1 h)\n",
        "\n",
        "data = data[data['column_mmsi'] == True] #drop rows where column id is false, id(t)=/=id(t-1)\n",
        "data['column_mmsi'] = np.where(data[\"mmsi(t-1)\"] == data[\"mmsi(t)\"], True, False)\n",
        "false_count = (~data.column_mmsi).sum()\n",
        "true_count = (data.column_mmsi).sum()\n",
        "print(\"num of false mmsi\",(false_count))\n",
        "print(\"num of true mmsi\",(true_count))\n",
        "\n",
        "data['column_id'] = np.where(data[\"id(t-1)\"] == data[\"id(t)\"], True, False)\n",
        "false_count_id = (~data.column_id).sum()\n",
        "true_count_id = (data.column_id).sum()\n",
        "print(\"num of false id\",(false_count_id))\n",
        "print(\"num of true id\",(true_count_id))\n",
        "print(\"now mmsi(t) == mmsi(t-1) \")\n",
        "#shift me group, short me xrono\n",
        "\n",
        "df0=data.copy()\n",
        "df0 = df0[df0.columns.drop(list(df0.filter(regex='mmsi')))]\n",
        "df0 = df0[df0.columns.drop(list(df0.filter(regex='dataset_tr1_val2_test3_augm_0_norm')))]\n",
        "df0 = df0[df0.columns.drop(list(df0.filter(regex='id')))]\n",
        "df0 = df0[df0.columns.drop(list(df0.filter(regex='WGS84')))]\n",
        "df0 = df0[df0.columns.drop(list(df0.filter(regex='dist_m')))]\n",
        "\n",
        "\n",
        "df0=df0.drop(labels=['t(t-10)', 't(t-9)','t(t-8)','t(t-7)',\n",
        "                 't(t-6)','t(t-5)','t(t-4)', 't(t-3)','t(t-2)','t(t-1)','t(t)','dt(t-10)'], axis=1) \n",
        "\n",
        "df0=df0.drop(labels=['dataset_tr1_val2_test3_augm_0(t-9)', 'dataset_tr1_val2_test3_augm_0(t-8)',\n",
        "                     'dataset_tr1_val2_test3_augm_0(t-7)','dataset_tr1_val2_test3_augm_0(t-6)',\n",
        "                 'dataset_tr1_val2_test3_augm_0(t-5)','dataset_tr1_val2_test3_augm_0(t-4)',\n",
        "                     'dataset_tr1_val2_test3_augm_0(t-3)', 'dataset_tr1_val2_test3_augm_0(t-2)',\n",
        "                     'dataset_tr1_val2_test3_augm_0(t-1)','dataset_tr1_val2_test3_augm_0(t)'], axis=1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "Lp4ERu5G6JKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4473a2-dc83-4acf-ed4f-42cd5962d0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num of false mmsi 0\n",
            "num of true mmsi 57616\n",
            "num of false id 0\n",
            "num of true id 57616\n",
            "num of false mmsi 0\n",
            "num of true mmsi 57616\n",
            "num of false id 0\n",
            "num of true id 57616\n",
            "now mmsi(t) == mmsi(t-1) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Normalisation\n",
        "\n"
      ],
      "metadata": {
        "id": "xf6B4lYKeHlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster=0\n",
        "grouped=df0.groupby(df0['dataset_tr1_val2_test3_augm_0(t-10)'])\n",
        "df_tr = grouped.get_group(1.0)\n",
        "grouped_cluster=df_tr.groupby(df_tr['cluster'])\n",
        "df_tr = grouped_cluster.get_group(cluster)\n",
        "\n",
        "# Create a function to generate synthetic data\n",
        "def generate_synthetic_data(df, num_rows):\n",
        "    mean = df.mean()\n",
        "    std = df.std()\n",
        "    synthetic_data = []\n",
        "    for i in range(num_rows):\n",
        "        synthetic_row = []\n",
        "        for col_mean, col_std in zip(mean, std):\n",
        "            synthetic_value = np.random.normal(loc=col_mean, scale=(col_std/4))\n",
        "            synthetic_row.append(synthetic_value)\n",
        "        synthetic_data.append(synthetic_row)\n",
        "    return pd.DataFrame(synthetic_data, columns=df.columns)\n",
        "# usage\n",
        "synthetic_data = generate_synthetic_data(df_tr, 5000)\n",
        "df_tr = df_tr.append(synthetic_data, ignore_index=True)\n",
        "\n",
        "df_tr1=df_tr.copy()\n",
        "#to thelw gia to telos gia na upologisw ta lon-lat(t)\n",
        "dftrainzlon=df_tr1[[\"lon(t-1)\"]]\n",
        "dftrainzlat=df_tr1[[\"lat(t-1)\"]]\n",
        "df_trx=df_tr.filter(regex=(\"^dlon|^dlat*|^speed|^dt\"))\n",
        "df_trx=df_trx.drop(labels=['dlon(t)', 'dlat(t)'], axis=1)\n",
        "df_try=df_tr1[[\"dlon(t)\",\"dlat(t)\"]]\n",
        "\n",
        "df_val= grouped.get_group(2.0)\n",
        "grouped_cluster=df_val.groupby(df_val['cluster'])\n",
        "df_val = grouped_cluster.get_group(cluster)\n",
        "df_val1=df_val.copy()\n",
        "#gia to telos \n",
        "dfvalzlon=df_val1[[\"lon(t-1)\"]]\n",
        "dfvalzlat=df_val1[[\"lat(t-1)\"]]\n",
        "\n",
        "df_valx=df_val.filter(regex=(\"^dlon|^dlat*|^speed|^dt\"))\n",
        "df_valx=df_valx.drop(labels=['dlon(t)', 'dlat(t)'], axis=1)\n",
        "df_valy=df_val1[[\"dlon(t)\",\"dlat(t)\"]]\n",
        "\n",
        "df_test = grouped.get_group(3.0)\n",
        "grouped_cluster=df_test.groupby(df_test['cluster'])\n",
        "df_test = grouped_cluster.get_group(cluster)\n",
        "df_test1=df_test.copy()\n",
        "#to thelw gia to telos\n",
        "dftestzlon=df_test1[[\"lon(t-1)\"]]\n",
        "dftestzlat=df_test1[[\"lat(t-1)\"]]\n",
        "\n",
        "df_testx=df_test.filter(regex=(\"^dlon|^dlat*|^speed|^dt\"))\n",
        "df_testx=df_testx.drop(labels=['dlon(t)', 'dlat(t)'], axis=1)\n",
        "df_testy=df_test1[[\"dlon(t)\",\"dlat(t)\"]]\n",
        "\n",
        "##### TENSOR CREATION\n",
        "\n",
        "\n",
        "df_trx['tensor'] = df_trx['dlon(t-1)']/df_trx['dlat(t-1)']\n",
        "\n",
        "\n",
        "df_valx['tensor'] = df_valx['dlon(t-1)']/df_valx['dlat(t-1)']\n",
        "\n",
        "df_testx['tensor'] = df_testx['dlon(t-1)']/df_testx['dlat(t-1)']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iXVXdZcjeEcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> #  Splitting Data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R675Nm4bfHn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_trx1=df_trx.copy()\n",
        "df_try1=df_try.copy()\n",
        "df_valx1=df_valx.copy()\n",
        "df_valy1=df_valy.copy()\n",
        "df_testx1=df_testx.copy()\n",
        "df_testy1=df_testy.copy()\n",
        "\n",
        "df_lon_trx = df_trx[df_trx.columns.drop(list(df_trx.filter(regex='dlat')))]\n",
        "df_lon_try=df_try.drop(labels=['dlat(t)'], axis=1)\n",
        "\n",
        "df_lon_valx = df_valx[df_valx.columns.drop(list(df_valx.filter(regex='dlat')))]\n",
        "df_lon_valy=df_valy.drop(labels=['dlat(t)'], axis=1)\n",
        "\n",
        "df_lon_testx = df_testx[df_testx.columns.drop(list(df_testx.filter(regex='dlat')))]\n",
        "df_lon_testy=df_testy.drop(labels=['dlat(t)'], axis=1)\n",
        "\n",
        "df_lat_trx = df_trx1[df_trx1.columns.drop(list(df_trx1.filter(regex='dlon')))]\n",
        "df_lat_try=df_try1.drop(labels=['dlon(t)'], axis=1)\n",
        "\n",
        "df_lat_valx = df_valx1[df_valx1.columns.drop(list(df_valx1.filter(regex='dlon')))]\n",
        "df_lat_valy=df_valy1.drop(labels=['dlon(t)'], axis=1)\n",
        "\n",
        "df_lat_testx = df_testx1[df_testx1.columns.drop(list(df_testx1.filter(regex='dlon')))]\n",
        "df_lat_testy=df_testy1.drop(labels=['dlon(t)'], axis=1)"
      ],
      "metadata": {
        "id": "e2cHNjr6e61y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for col in df_lon_trx.columns:\n",
        "    print(col)\n",
        "    i=i+1\n",
        "print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOYGPANrnxAN",
        "outputId": "2ecac740-a35a-49c5-9304-89481a0022ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dlon(t-10)\n",
            "speed(t-10)\n",
            "dlon(t-9)\n",
            "dt(t-9)\n",
            "speed(t-9)\n",
            "dlon(t-8)\n",
            "dt(t-8)\n",
            "speed(t-8)\n",
            "dlon(t-7)\n",
            "dt(t-7)\n",
            "speed(t-7)\n",
            "dlon(t-6)\n",
            "dt(t-6)\n",
            "speed(t-6)\n",
            "dlon(t-5)\n",
            "dt(t-5)\n",
            "speed(t-5)\n",
            "dlon(t-4)\n",
            "dt(t-4)\n",
            "speed(t-4)\n",
            "dlon(t-3)\n",
            "dt(t-3)\n",
            "speed(t-3)\n",
            "dlon(t-2)\n",
            "dt(t-2)\n",
            "speed(t-2)\n",
            "dlon(t-1)\n",
            "dt(t-1)\n",
            "speed(t-1)\n",
            "dt(t)\n",
            "speed(t)\n",
            "tensor\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "#MinMaxScaler(feature_range=(-1, 1)),StandardScaler()\n",
        "scaler1 = StandardScaler()\n",
        "scaler2 = StandardScaler()\n",
        "scaler3 = StandardScaler()\n",
        "scaler4 = StandardScaler()\n",
        "scaler5 = StandardScaler()\n",
        "scaler6 = StandardScaler()\n",
        "scaler7 = StandardScaler()\n",
        "scaler8 = StandardScaler()\n",
        "scaler9 = StandardScaler()\n",
        "scaler10 = StandardScaler()\n",
        "scaler11 = StandardScaler()\n",
        "scaler12 = StandardScaler()\n",
        "\n",
        "\n",
        "#lon scaling\n",
        "columnslon=['tensor','speed(t-10)','speed(t-9)','speed(t-8)','speed(t-7)','speed(t-6)','speed(t-5)','speed(t-4)','speed(t-3)','speed(t-2)','speed(t-1)','speed(t)','dlon(t-10)','dlon(t-9)','dt(t-9)','dlon(t-8)','dt(t-8)','dlon(t-7)','dt(t-7)','dlon(t-6)','dt(t-6)','dlon(t-5)','dt(t-5)','dlon(t-4)','dt(t-4)','dlon(t-3)','dt(t-3)','dlon(t-2)','dt(t-2)','dlon(t-1)','dt(t-1)','dt(t)']\n",
        "columnslat=['tensor','speed(t-10)','speed(t-9)','speed(t-8)','speed(t-7)','speed(t-6)','speed(t-5)','speed(t-4)','speed(t-3)','speed(t-2)','speed(t-1)','speed(t)','dlat(t-10)','dlat(t-9)','dt(t-9)','dlat(t-8)','dt(t-8)','dlat(t-7)','dt(t-7)','dlat(t-6)','dt(t-6)','dlat(t-5)','dt(t-5)','dlat(t-4)','dt(t-4)','dlat(t-3)','dt(t-3)','dlat(t-2)','dt(t-2)','dlat(t-1)','dt(t-1)','dt(t)']\n",
        "\n",
        "df_lon_trx[columnslon]=scaler1.fit_transform(df_lon_trx[columnslon])\n",
        "df_lon_try[['dlon(t)']]=scaler2.fit_transform(df_lon_try[['dlon(t)']])\n",
        "df_lon_valx[columnslon]=scaler3.fit_transform(df_lon_valx[columnslon])\n",
        "df_lon_valy[['dlon(t)']]=scaler4.fit_transform(df_lon_valy[['dlon(t)']])\n",
        "df_lon_testx[columnslon]=scaler5.fit_transform(df_lon_testx[columnslon])\n",
        "df_lon_testy[['dlon(t)']]=scaler6.fit_transform(df_lon_testy[['dlon(t)']])\n",
        "\n",
        "df_lat_trx[columnslat]=scaler7.fit_transform(df_lat_trx[columnslat])\n",
        "df_lat_try[['dlat(t)']]=scaler8.fit_transform(df_lat_try[['dlat(t)']])\n",
        "df_lat_valx[columnslat]=scaler9.fit_transform(df_lat_valx[columnslat])\n",
        "df_lat_valy[['dlat(t)']]=scaler10.fit_transform(df_lat_valy[['dlat(t)']])\n",
        "df_lat_testx[columnslat]=scaler11.fit_transform(df_lat_testx[columnslat])\n",
        "df_lat_testy[['dlat(t)']]=scaler12.fit_transform(df_lat_testy[['dlat(t)']])"
      ],
      "metadata": {
        "id": "tlFMkMuceywE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# > Model Dlon\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VYxUW66qg29o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a model with SimpleRNN \n",
        "layer1=\"relu\"\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from tensorflow.keras.layers import (\n",
        "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
        ")\n",
        "#lets try Dropout BatchNormalization shuflle  relu elu esu \n",
        "#define model and model parameters\n",
        "batch_size = 64\n",
        "model_lon = Sequential()\n",
        "n_cols=df_lon_trx.shape[1]\n",
        "epochs=10000\n",
        "patience=15\n",
        "\n",
        "model_lon.add(Dense(units=32, input_shape=(n_cols,), activation=layer1)) \n",
        "model_lon.add(Dense(8, activation=layer1))\n",
        "model_lon.add(Dense(8, activation=layer1))\n",
        "model_lon.add(Dense(4, activation=layer1)) \n",
        "model_lon.add(Dense(2, activation=layer1))\n",
        "model_lon.add(Dense(2, activation=layer1))\n",
        "model_lon.add(Dense(2, activation=layer1))\n",
        "model_lon.add(Dense(1))\n",
        "\n",
        "#compile model\n",
        "model_lon.compile(loss='mse', optimizer='adam',metrics=['mean_squared_error']) \n",
        "#fit model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpointer = ModelCheckpoint(filepath='lon_weights.hdf5', \n",
        "                               save_best_only=True, save_weights_only=True, \n",
        "                               monitor='val_mean_squared_error', mode='min', \n",
        "                               verbose=1)\n",
        "early_stopping_monitor = EarlyStopping(patience=patience)\n",
        "history=model_lon.fit(df_lon_trx, df_lon_try, validation_data=(df_lon_testx,df_lon_testy), epochs=epochs, \n",
        "                  callbacks=[early_stopping_monitor,checkpointer], batch_size=batch_size, verbose=1,shuffle=True)\n",
        "model_lon.summary()\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "#mean_squared_error: 0.0430 me ola\n",
        "#mean_squared_error: 0.0080  xwris batch normalisation to error anebhke apo 19km sta 30km (5 epochs)\n",
        " \n",
        "\n",
        "\n",
        "plt.plot(history.history['mean_squared_error'])\n",
        "plt.plot(history.history['val_mean_squared_error'])\n",
        "plt.title('model mean_squared_error')\n",
        "plt.ylabel('mean_squared_error')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v9zDvr_qg9fs",
        "outputId": "bd543f56-c6f1-40ea-9d35-ca8bc7112450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.7195 - mean_squared_error: 0.7195\n",
            "Epoch 1: val_mean_squared_error improved from inf to 0.46883, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 3s 4ms/step - loss: 0.7195 - mean_squared_error: 0.7195 - val_loss: 0.4688 - val_mean_squared_error: 0.4688\n",
            "Epoch 2/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.5326 - mean_squared_error: 0.5326\n",
            "Epoch 2: val_mean_squared_error improved from 0.46883 to 0.31720, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.5205 - mean_squared_error: 0.5205 - val_loss: 0.3172 - val_mean_squared_error: 0.3172\n",
            "Epoch 3/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.4186 - mean_squared_error: 0.4186\n",
            "Epoch 3: val_mean_squared_error improved from 0.31720 to 0.23831, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.4140 - mean_squared_error: 0.4140 - val_loss: 0.2383 - val_mean_squared_error: 0.2383\n",
            "Epoch 4/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.3598 - mean_squared_error: 0.3598\n",
            "Epoch 4: val_mean_squared_error improved from 0.23831 to 0.19419, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.1942 - val_mean_squared_error: 0.1942\n",
            "Epoch 5/10000\n",
            "245/256 [===========================>..] - ETA: 0s - loss: 0.3216 - mean_squared_error: 0.3216\n",
            "Epoch 5: val_mean_squared_error improved from 0.19419 to 0.17487, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.3225 - mean_squared_error: 0.3225 - val_loss: 0.1749 - val_mean_squared_error: 0.1749\n",
            "Epoch 6/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.2983 - mean_squared_error: 0.2983\n",
            "Epoch 6: val_mean_squared_error improved from 0.17487 to 0.15982, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.3001 - mean_squared_error: 0.3001 - val_loss: 0.1598 - val_mean_squared_error: 0.1598\n",
            "Epoch 7/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.2913 - mean_squared_error: 0.2913\n",
            "Epoch 7: val_mean_squared_error improved from 0.15982 to 0.15148, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2837 - mean_squared_error: 0.2837 - val_loss: 0.1515 - val_mean_squared_error: 0.1515\n",
            "Epoch 8/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.2623 - mean_squared_error: 0.2623\n",
            "Epoch 8: val_mean_squared_error improved from 0.15148 to 0.13865, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2731 - mean_squared_error: 0.2731 - val_loss: 0.1387 - val_mean_squared_error: 0.1387\n",
            "Epoch 9/10000\n",
            "236/256 [==========================>...] - ETA: 0s - loss: 0.2478 - mean_squared_error: 0.2478\n",
            "Epoch 9: val_mean_squared_error improved from 0.13865 to 0.13645, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2667 - mean_squared_error: 0.2667 - val_loss: 0.1365 - val_mean_squared_error: 0.1365\n",
            "Epoch 10/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.2591 - mean_squared_error: 0.2591\n",
            "Epoch 10: val_mean_squared_error improved from 0.13645 to 0.13030, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2596 - mean_squared_error: 0.2596 - val_loss: 0.1303 - val_mean_squared_error: 0.1303\n",
            "Epoch 11/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.2453 - mean_squared_error: 0.2453\n",
            "Epoch 11: val_mean_squared_error improved from 0.13030 to 0.12570, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2530 - mean_squared_error: 0.2530 - val_loss: 0.1257 - val_mean_squared_error: 0.1257\n",
            "Epoch 12/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.2492 - mean_squared_error: 0.2492\n",
            "Epoch 12: val_mean_squared_error improved from 0.12570 to 0.12056, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2479 - mean_squared_error: 0.2479 - val_loss: 0.1206 - val_mean_squared_error: 0.1206\n",
            "Epoch 13/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.2456 - mean_squared_error: 0.2456\n",
            "Epoch 13: val_mean_squared_error improved from 0.12056 to 0.11599, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2416 - mean_squared_error: 0.2416 - val_loss: 0.1160 - val_mean_squared_error: 0.1160\n",
            "Epoch 14/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.2285 - mean_squared_error: 0.2285\n",
            "Epoch 14: val_mean_squared_error improved from 0.11599 to 0.11552, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2387 - mean_squared_error: 0.2387 - val_loss: 0.1155 - val_mean_squared_error: 0.1155\n",
            "Epoch 15/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.2209 - mean_squared_error: 0.2209\n",
            "Epoch 15: val_mean_squared_error improved from 0.11552 to 0.10819, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2325 - mean_squared_error: 0.2325 - val_loss: 0.1082 - val_mean_squared_error: 0.1082\n",
            "Epoch 16/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.2386 - mean_squared_error: 0.2386\n",
            "Epoch 16: val_mean_squared_error improved from 0.10819 to 0.10693, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2272 - mean_squared_error: 0.2272 - val_loss: 0.1069 - val_mean_squared_error: 0.1069\n",
            "Epoch 17/10000\n",
            "255/256 [============================>.] - ETA: 0s - loss: 0.2236 - mean_squared_error: 0.2236\n",
            "Epoch 17: val_mean_squared_error improved from 0.10693 to 0.10611, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2233 - mean_squared_error: 0.2233 - val_loss: 0.1061 - val_mean_squared_error: 0.1061\n",
            "Epoch 18/10000\n",
            "235/256 [==========================>...] - ETA: 0s - loss: 0.2114 - mean_squared_error: 0.2114\n",
            "Epoch 18: val_mean_squared_error improved from 0.10611 to 0.10233, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2202 - mean_squared_error: 0.2202 - val_loss: 0.1023 - val_mean_squared_error: 0.1023\n",
            "Epoch 19/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.2142 - mean_squared_error: 0.2142\n",
            "Epoch 19: val_mean_squared_error improved from 0.10233 to 0.09870, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2152 - mean_squared_error: 0.2152 - val_loss: 0.0987 - val_mean_squared_error: 0.0987\n",
            "Epoch 20/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.2203 - mean_squared_error: 0.2203\n",
            "Epoch 20: val_mean_squared_error did not improve from 0.09870\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2112 - mean_squared_error: 0.2112 - val_loss: 0.1023 - val_mean_squared_error: 0.1023\n",
            "Epoch 21/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.2020 - mean_squared_error: 0.2020\n",
            "Epoch 21: val_mean_squared_error improved from 0.09870 to 0.09580, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2078 - mean_squared_error: 0.2078 - val_loss: 0.0958 - val_mean_squared_error: 0.0958\n",
            "Epoch 22/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.2106 - mean_squared_error: 0.2106\n",
            "Epoch 22: val_mean_squared_error did not improve from 0.09580\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2049 - mean_squared_error: 0.2049 - val_loss: 0.0965 - val_mean_squared_error: 0.0965\n",
            "Epoch 23/10000\n",
            "244/256 [===========================>..] - ETA: 0s - loss: 0.2051 - mean_squared_error: 0.2051\n",
            "Epoch 23: val_mean_squared_error improved from 0.09580 to 0.09567, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2014 - mean_squared_error: 0.2014 - val_loss: 0.0957 - val_mean_squared_error: 0.0957\n",
            "Epoch 24/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1966 - mean_squared_error: 0.1966\n",
            "Epoch 24: val_mean_squared_error did not improve from 0.09567\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1994 - mean_squared_error: 0.1994 - val_loss: 0.0978 - val_mean_squared_error: 0.0978\n",
            "Epoch 25/10000\n",
            "235/256 [==========================>...] - ETA: 0s - loss: 0.2014 - mean_squared_error: 0.2014\n",
            "Epoch 25: val_mean_squared_error improved from 0.09567 to 0.09427, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1951 - mean_squared_error: 0.1951 - val_loss: 0.0943 - val_mean_squared_error: 0.0943\n",
            "Epoch 26/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1915 - mean_squared_error: 0.1915\n",
            "Epoch 26: val_mean_squared_error improved from 0.09427 to 0.08915, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1927 - mean_squared_error: 0.1927 - val_loss: 0.0892 - val_mean_squared_error: 0.0892\n",
            "Epoch 27/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1956 - mean_squared_error: 0.1956\n",
            "Epoch 27: val_mean_squared_error did not improve from 0.08915\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1893 - mean_squared_error: 0.1893 - val_loss: 0.0895 - val_mean_squared_error: 0.0895\n",
            "Epoch 28/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1966 - mean_squared_error: 0.1966\n",
            "Epoch 28: val_mean_squared_error improved from 0.08915 to 0.08549, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1882 - mean_squared_error: 0.1882 - val_loss: 0.0855 - val_mean_squared_error: 0.0855\n",
            "Epoch 29/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1921 - mean_squared_error: 0.1921\n",
            "Epoch 29: val_mean_squared_error did not improve from 0.08549\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1843 - mean_squared_error: 0.1843 - val_loss: 0.0915 - val_mean_squared_error: 0.0915\n",
            "Epoch 30/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1764 - mean_squared_error: 0.1764\n",
            "Epoch 30: val_mean_squared_error did not improve from 0.08549\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1819 - mean_squared_error: 0.1819 - val_loss: 0.0871 - val_mean_squared_error: 0.0871\n",
            "Epoch 31/10000\n",
            "254/256 [============================>.] - ETA: 0s - loss: 0.1804 - mean_squared_error: 0.1804\n",
            "Epoch 31: val_mean_squared_error improved from 0.08549 to 0.08505, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1796 - mean_squared_error: 0.1796 - val_loss: 0.0851 - val_mean_squared_error: 0.0851\n",
            "Epoch 32/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1707 - mean_squared_error: 0.1707\n",
            "Epoch 32: val_mean_squared_error did not improve from 0.08505\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1802 - mean_squared_error: 0.1802 - val_loss: 0.0861 - val_mean_squared_error: 0.0861\n",
            "Epoch 33/10000\n",
            "234/256 [==========================>...] - ETA: 0s - loss: 0.1817 - mean_squared_error: 0.1817\n",
            "Epoch 33: val_mean_squared_error improved from 0.08505 to 0.08458, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1756 - mean_squared_error: 0.1756 - val_loss: 0.0846 - val_mean_squared_error: 0.0846\n",
            "Epoch 34/10000\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.1728 - mean_squared_error: 0.1728\n",
            "Epoch 34: val_mean_squared_error improved from 0.08458 to 0.08389, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1728 - mean_squared_error: 0.1728 - val_loss: 0.0839 - val_mean_squared_error: 0.0839\n",
            "Epoch 35/10000\n",
            "253/256 [============================>.] - ETA: 0s - loss: 0.1720 - mean_squared_error: 0.1720\n",
            "Epoch 35: val_mean_squared_error improved from 0.08389 to 0.08145, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1710 - mean_squared_error: 0.1710 - val_loss: 0.0815 - val_mean_squared_error: 0.0815\n",
            "Epoch 36/10000\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.1699 - mean_squared_error: 0.1699\n",
            "Epoch 36: val_mean_squared_error did not improve from 0.08145\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1699 - mean_squared_error: 0.1699 - val_loss: 0.0842 - val_mean_squared_error: 0.0842\n",
            "Epoch 37/10000\n",
            "244/256 [===========================>..] - ETA: 0s - loss: 0.1712 - mean_squared_error: 0.1712\n",
            "Epoch 37: val_mean_squared_error did not improve from 0.08145\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1672 - mean_squared_error: 0.1672 - val_loss: 0.0829 - val_mean_squared_error: 0.0829\n",
            "Epoch 38/10000\n",
            "235/256 [==========================>...] - ETA: 0s - loss: 0.1605 - mean_squared_error: 0.1605\n",
            "Epoch 38: val_mean_squared_error improved from 0.08145 to 0.08024, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1648 - mean_squared_error: 0.1648 - val_loss: 0.0802 - val_mean_squared_error: 0.0802\n",
            "Epoch 39/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1713 - mean_squared_error: 0.1713\n",
            "Epoch 39: val_mean_squared_error improved from 0.08024 to 0.07787, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1642 - mean_squared_error: 0.1642 - val_loss: 0.0779 - val_mean_squared_error: 0.0779\n",
            "Epoch 40/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.1423 - mean_squared_error: 0.1423\n",
            "Epoch 40: val_mean_squared_error did not improve from 0.07787\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1621 - mean_squared_error: 0.1621 - val_loss: 0.0795 - val_mean_squared_error: 0.0795\n",
            "Epoch 41/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1558 - mean_squared_error: 0.1558\n",
            "Epoch 41: val_mean_squared_error did not improve from 0.07787\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1597 - mean_squared_error: 0.1597 - val_loss: 0.0796 - val_mean_squared_error: 0.0796\n",
            "Epoch 42/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1657 - mean_squared_error: 0.1657\n",
            "Epoch 42: val_mean_squared_error improved from 0.07787 to 0.07657, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1582 - mean_squared_error: 0.1582 - val_loss: 0.0766 - val_mean_squared_error: 0.0766\n",
            "Epoch 43/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1479 - mean_squared_error: 0.1479\n",
            "Epoch 43: val_mean_squared_error did not improve from 0.07657\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1572 - mean_squared_error: 0.1572 - val_loss: 0.0809 - val_mean_squared_error: 0.0809\n",
            "Epoch 44/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1613 - mean_squared_error: 0.1613\n",
            "Epoch 44: val_mean_squared_error improved from 0.07657 to 0.07577, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1557 - mean_squared_error: 0.1557 - val_loss: 0.0758 - val_mean_squared_error: 0.0758\n",
            "Epoch 45/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.1582 - mean_squared_error: 0.1582\n",
            "Epoch 45: val_mean_squared_error improved from 0.07577 to 0.07537, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1544 - mean_squared_error: 0.1544 - val_loss: 0.0754 - val_mean_squared_error: 0.0754\n",
            "Epoch 46/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.1557 - mean_squared_error: 0.1557\n",
            "Epoch 46: val_mean_squared_error did not improve from 0.07537\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1523 - mean_squared_error: 0.1523 - val_loss: 0.0779 - val_mean_squared_error: 0.0779\n",
            "Epoch 47/10000\n",
            "248/256 [============================>.] - ETA: 0s - loss: 0.1541 - mean_squared_error: 0.1541\n",
            "Epoch 47: val_mean_squared_error did not improve from 0.07537\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1508 - mean_squared_error: 0.1508 - val_loss: 0.0781 - val_mean_squared_error: 0.0781\n",
            "Epoch 48/10000\n",
            "255/256 [============================>.] - ETA: 0s - loss: 0.1502 - mean_squared_error: 0.1502\n",
            "Epoch 48: val_mean_squared_error did not improve from 0.07537\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1500 - mean_squared_error: 0.1500 - val_loss: 0.0759 - val_mean_squared_error: 0.0759\n",
            "Epoch 49/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1531 - mean_squared_error: 0.1531\n",
            "Epoch 49: val_mean_squared_error improved from 0.07537 to 0.07477, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1490 - mean_squared_error: 0.1490 - val_loss: 0.0748 - val_mean_squared_error: 0.0748\n",
            "Epoch 50/10000\n",
            "234/256 [==========================>...] - ETA: 0s - loss: 0.1388 - mean_squared_error: 0.1388\n",
            "Epoch 50: val_mean_squared_error improved from 0.07477 to 0.07312, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1460 - mean_squared_error: 0.1460 - val_loss: 0.0731 - val_mean_squared_error: 0.0731\n",
            "Epoch 51/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.1522 - mean_squared_error: 0.1522\n",
            "Epoch 51: val_mean_squared_error did not improve from 0.07312\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1447 - mean_squared_error: 0.1447 - val_loss: 0.0739 - val_mean_squared_error: 0.0739\n",
            "Epoch 52/10000\n",
            "236/256 [==========================>...] - ETA: 0s - loss: 0.1410 - mean_squared_error: 0.1410\n",
            "Epoch 52: val_mean_squared_error did not improve from 0.07312\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1444 - mean_squared_error: 0.1444 - val_loss: 0.0745 - val_mean_squared_error: 0.0745\n",
            "Epoch 53/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.1439 - mean_squared_error: 0.1439\n",
            "Epoch 53: val_mean_squared_error did not improve from 0.07312\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1425 - mean_squared_error: 0.1425 - val_loss: 0.0847 - val_mean_squared_error: 0.0847\n",
            "Epoch 54/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1475 - mean_squared_error: 0.1475\n",
            "Epoch 54: val_mean_squared_error improved from 0.07312 to 0.07274, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1421 - mean_squared_error: 0.1421 - val_loss: 0.0727 - val_mean_squared_error: 0.0727\n",
            "Epoch 55/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.1322 - mean_squared_error: 0.1322\n",
            "Epoch 55: val_mean_squared_error improved from 0.07274 to 0.07272, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1409 - mean_squared_error: 0.1409 - val_loss: 0.0727 - val_mean_squared_error: 0.0727\n",
            "Epoch 56/10000\n",
            "234/256 [==========================>...] - ETA: 0s - loss: 0.1469 - mean_squared_error: 0.1469\n",
            "Epoch 56: val_mean_squared_error improved from 0.07272 to 0.07096, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1392 - mean_squared_error: 0.1392 - val_loss: 0.0710 - val_mean_squared_error: 0.0710\n",
            "Epoch 57/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1326 - mean_squared_error: 0.1326\n",
            "Epoch 57: val_mean_squared_error improved from 0.07096 to 0.07035, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1377 - mean_squared_error: 0.1377 - val_loss: 0.0704 - val_mean_squared_error: 0.0704\n",
            "Epoch 58/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1132 - mean_squared_error: 0.1132\n",
            "Epoch 58: val_mean_squared_error did not improve from 0.07035\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1381 - mean_squared_error: 0.1381 - val_loss: 0.0711 - val_mean_squared_error: 0.0711\n",
            "Epoch 59/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1345 - mean_squared_error: 0.1345\n",
            "Epoch 59: val_mean_squared_error did not improve from 0.07035\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1355 - mean_squared_error: 0.1355 - val_loss: 0.0763 - val_mean_squared_error: 0.0763\n",
            "Epoch 60/10000\n",
            "236/256 [==========================>...] - ETA: 0s - loss: 0.1365 - mean_squared_error: 0.1365\n",
            "Epoch 60: val_mean_squared_error did not improve from 0.07035\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1356 - mean_squared_error: 0.1356 - val_loss: 0.0775 - val_mean_squared_error: 0.0775\n",
            "Epoch 61/10000\n",
            "234/256 [==========================>...] - ETA: 0s - loss: 0.1418 - mean_squared_error: 0.1418\n",
            "Epoch 61: val_mean_squared_error improved from 0.07035 to 0.06833, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1339 - mean_squared_error: 0.1339 - val_loss: 0.0683 - val_mean_squared_error: 0.0683\n",
            "Epoch 62/10000\n",
            "234/256 [==========================>...] - ETA: 0s - loss: 0.1348 - mean_squared_error: 0.1348\n",
            "Epoch 62: val_mean_squared_error did not improve from 0.06833\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1325 - mean_squared_error: 0.1325 - val_loss: 0.0745 - val_mean_squared_error: 0.0745\n",
            "Epoch 63/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.1281 - mean_squared_error: 0.1281\n",
            "Epoch 63: val_mean_squared_error did not improve from 0.06833\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1329 - mean_squared_error: 0.1329 - val_loss: 0.0719 - val_mean_squared_error: 0.0719\n",
            "Epoch 64/10000\n",
            "244/256 [===========================>..] - ETA: 0s - loss: 0.1142 - mean_squared_error: 0.1142\n",
            "Epoch 64: val_mean_squared_error did not improve from 0.06833\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1307 - mean_squared_error: 0.1307 - val_loss: 0.0734 - val_mean_squared_error: 0.0734\n",
            "Epoch 65/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1290 - mean_squared_error: 0.1290\n",
            "Epoch 65: val_mean_squared_error did not improve from 0.06833\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1310 - mean_squared_error: 0.1310 - val_loss: 0.0738 - val_mean_squared_error: 0.0738\n",
            "Epoch 66/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1341 - mean_squared_error: 0.1341\n",
            "Epoch 66: val_mean_squared_error did not improve from 0.06833\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1290 - mean_squared_error: 0.1290 - val_loss: 0.0722 - val_mean_squared_error: 0.0722\n",
            "Epoch 67/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1148 - mean_squared_error: 0.1148\n",
            "Epoch 67: val_mean_squared_error improved from 0.06833 to 0.06711, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1287 - mean_squared_error: 0.1287 - val_loss: 0.0671 - val_mean_squared_error: 0.0671\n",
            "Epoch 68/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.1259 - mean_squared_error: 0.1259\n",
            "Epoch 68: val_mean_squared_error did not improve from 0.06711\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1266 - mean_squared_error: 0.1266 - val_loss: 0.0692 - val_mean_squared_error: 0.0692\n",
            "Epoch 69/10000\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.1277 - mean_squared_error: 0.1277\n",
            "Epoch 69: val_mean_squared_error did not improve from 0.06711\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1277 - mean_squared_error: 0.1277 - val_loss: 0.0682 - val_mean_squared_error: 0.0682\n",
            "Epoch 70/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1269 - mean_squared_error: 0.1269\n",
            "Epoch 70: val_mean_squared_error did not improve from 0.06711\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1273 - mean_squared_error: 0.1273 - val_loss: 0.0678 - val_mean_squared_error: 0.0678\n",
            "Epoch 71/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1287 - mean_squared_error: 0.1287\n",
            "Epoch 71: val_mean_squared_error did not improve from 0.06711\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1242 - mean_squared_error: 0.1242 - val_loss: 0.0692 - val_mean_squared_error: 0.0692\n",
            "Epoch 72/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.1285 - mean_squared_error: 0.1285\n",
            "Epoch 72: val_mean_squared_error did not improve from 0.06711\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1247 - mean_squared_error: 0.1247 - val_loss: 0.0681 - val_mean_squared_error: 0.0681\n",
            "Epoch 73/10000\n",
            "247/256 [===========================>..] - ETA: 0s - loss: 0.1195 - mean_squared_error: 0.1195\n",
            "Epoch 73: val_mean_squared_error did not improve from 0.06711\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1244 - mean_squared_error: 0.1244 - val_loss: 0.0706 - val_mean_squared_error: 0.0706\n",
            "Epoch 74/10000\n",
            "244/256 [===========================>..] - ETA: 0s - loss: 0.1218 - mean_squared_error: 0.1218\n",
            "Epoch 74: val_mean_squared_error improved from 0.06711 to 0.06499, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1235 - mean_squared_error: 0.1235 - val_loss: 0.0650 - val_mean_squared_error: 0.0650\n",
            "Epoch 75/10000\n",
            "236/256 [==========================>...] - ETA: 0s - loss: 0.1273 - mean_squared_error: 0.1273\n",
            "Epoch 75: val_mean_squared_error did not improve from 0.06499\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1211 - mean_squared_error: 0.1211 - val_loss: 0.0656 - val_mean_squared_error: 0.0656\n",
            "Epoch 76/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1245 - mean_squared_error: 0.1245\n",
            "Epoch 76: val_mean_squared_error did not improve from 0.06499\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1209 - mean_squared_error: 0.1209 - val_loss: 0.0686 - val_mean_squared_error: 0.0686\n",
            "Epoch 77/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.1055 - mean_squared_error: 0.1055\n",
            "Epoch 77: val_mean_squared_error did not improve from 0.06499\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1205 - mean_squared_error: 0.1205 - val_loss: 0.0763 - val_mean_squared_error: 0.0763\n",
            "Epoch 78/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1067 - mean_squared_error: 0.1067\n",
            "Epoch 78: val_mean_squared_error did not improve from 0.06499\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1205 - mean_squared_error: 0.1205 - val_loss: 0.0672 - val_mean_squared_error: 0.0672\n",
            "Epoch 79/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.1224 - mean_squared_error: 0.1224\n",
            "Epoch 79: val_mean_squared_error did not improve from 0.06499\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1181 - mean_squared_error: 0.1181 - val_loss: 0.0654 - val_mean_squared_error: 0.0654\n",
            "Epoch 80/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1232 - mean_squared_error: 0.1232\n",
            "Epoch 80: val_mean_squared_error improved from 0.06499 to 0.06350, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1194 - mean_squared_error: 0.1194 - val_loss: 0.0635 - val_mean_squared_error: 0.0635\n",
            "Epoch 81/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.1198 - mean_squared_error: 0.1198\n",
            "Epoch 81: val_mean_squared_error did not improve from 0.06350\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1173 - mean_squared_error: 0.1173 - val_loss: 0.0657 - val_mean_squared_error: 0.0657\n",
            "Epoch 82/10000\n",
            "246/256 [===========================>..] - ETA: 0s - loss: 0.1199 - mean_squared_error: 0.1199\n",
            "Epoch 82: val_mean_squared_error did not improve from 0.06350\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1174 - mean_squared_error: 0.1174 - val_loss: 0.0644 - val_mean_squared_error: 0.0644\n",
            "Epoch 83/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.1186 - mean_squared_error: 0.1186\n",
            "Epoch 83: val_mean_squared_error improved from 0.06350 to 0.06349, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1163 - mean_squared_error: 0.1163 - val_loss: 0.0635 - val_mean_squared_error: 0.0635\n",
            "Epoch 84/10000\n",
            "247/256 [===========================>..] - ETA: 0s - loss: 0.1172 - mean_squared_error: 0.1172\n",
            "Epoch 84: val_mean_squared_error did not improve from 0.06349\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1155 - mean_squared_error: 0.1155 - val_loss: 0.0640 - val_mean_squared_error: 0.0640\n",
            "Epoch 85/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1186 - mean_squared_error: 0.1186\n",
            "Epoch 85: val_mean_squared_error did not improve from 0.06349\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1149 - mean_squared_error: 0.1149 - val_loss: 0.0658 - val_mean_squared_error: 0.0658\n",
            "Epoch 86/10000\n",
            "235/256 [==========================>...] - ETA: 0s - loss: 0.1126 - mean_squared_error: 0.1126\n",
            "Epoch 86: val_mean_squared_error did not improve from 0.06349\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1153 - mean_squared_error: 0.1153 - val_loss: 0.0658 - val_mean_squared_error: 0.0658\n",
            "Epoch 87/10000\n",
            "235/256 [==========================>...] - ETA: 0s - loss: 0.1195 - mean_squared_error: 0.1195\n",
            "Epoch 87: val_mean_squared_error improved from 0.06349 to 0.06261, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1145 - mean_squared_error: 0.1145 - val_loss: 0.0626 - val_mean_squared_error: 0.0626\n",
            "Epoch 88/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1192 - mean_squared_error: 0.1192\n",
            "Epoch 88: val_mean_squared_error did not improve from 0.06261\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1147 - mean_squared_error: 0.1147 - val_loss: 0.0639 - val_mean_squared_error: 0.0639\n",
            "Epoch 89/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1148 - mean_squared_error: 0.1148\n",
            "Epoch 89: val_mean_squared_error improved from 0.06261 to 0.06241, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1128 - mean_squared_error: 0.1128 - val_loss: 0.0624 - val_mean_squared_error: 0.0624\n",
            "Epoch 90/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1134 - mean_squared_error: 0.1134\n",
            "Epoch 90: val_mean_squared_error improved from 0.06241 to 0.05962, saving model to lon_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1111 - mean_squared_error: 0.1111 - val_loss: 0.0596 - val_mean_squared_error: 0.0596\n",
            "Epoch 91/10000\n",
            "235/256 [==========================>...] - ETA: 0s - loss: 0.1065 - mean_squared_error: 0.1065\n",
            "Epoch 91: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1114 - mean_squared_error: 0.1114 - val_loss: 0.0611 - val_mean_squared_error: 0.0611\n",
            "Epoch 92/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1107 - mean_squared_error: 0.1107\n",
            "Epoch 92: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1103 - mean_squared_error: 0.1103 - val_loss: 0.0639 - val_mean_squared_error: 0.0639\n",
            "Epoch 93/10000\n",
            "235/256 [==========================>...] - ETA: 0s - loss: 0.1117 - mean_squared_error: 0.1117\n",
            "Epoch 93: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1102 - mean_squared_error: 0.1102 - val_loss: 0.0658 - val_mean_squared_error: 0.0658\n",
            "Epoch 94/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1147 - mean_squared_error: 0.1147\n",
            "Epoch 94: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1110 - mean_squared_error: 0.1110 - val_loss: 0.0641 - val_mean_squared_error: 0.0641\n",
            "Epoch 95/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.1077 - mean_squared_error: 0.1077\n",
            "Epoch 95: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1102 - mean_squared_error: 0.1102 - val_loss: 0.0641 - val_mean_squared_error: 0.0641\n",
            "Epoch 96/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1128 - mean_squared_error: 0.1128\n",
            "Epoch 96: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1084 - mean_squared_error: 0.1084 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n",
            "Epoch 97/10000\n",
            "245/256 [===========================>..] - ETA: 0s - loss: 0.1108 - mean_squared_error: 0.1108\n",
            "Epoch 97: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1080 - mean_squared_error: 0.1080 - val_loss: 0.0660 - val_mean_squared_error: 0.0660\n",
            "Epoch 98/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1128 - mean_squared_error: 0.1128\n",
            "Epoch 98: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1090 - mean_squared_error: 0.1090 - val_loss: 0.0671 - val_mean_squared_error: 0.0671\n",
            "Epoch 99/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.1096 - mean_squared_error: 0.1096\n",
            "Epoch 99: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1064 - mean_squared_error: 0.1064 - val_loss: 0.0636 - val_mean_squared_error: 0.0636\n",
            "Epoch 100/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1022 - mean_squared_error: 0.1022\n",
            "Epoch 100: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1069 - mean_squared_error: 0.1069 - val_loss: 0.0619 - val_mean_squared_error: 0.0619\n",
            "Epoch 101/10000\n",
            "245/256 [===========================>..] - ETA: 0s - loss: 0.1021 - mean_squared_error: 0.1021\n",
            "Epoch 101: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1076 - mean_squared_error: 0.1076 - val_loss: 0.0674 - val_mean_squared_error: 0.0674\n",
            "Epoch 102/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1087 - mean_squared_error: 0.1087\n",
            "Epoch 102: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1055 - mean_squared_error: 0.1055 - val_loss: 0.0600 - val_mean_squared_error: 0.0600\n",
            "Epoch 103/10000\n",
            "235/256 [==========================>...] - ETA: 0s - loss: 0.1106 - mean_squared_error: 0.1106\n",
            "Epoch 103: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1051 - mean_squared_error: 0.1051 - val_loss: 0.0629 - val_mean_squared_error: 0.0629\n",
            "Epoch 104/10000\n",
            "236/256 [==========================>...] - ETA: 0s - loss: 0.1104 - mean_squared_error: 0.1104\n",
            "Epoch 104: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1052 - mean_squared_error: 0.1052 - val_loss: 0.0613 - val_mean_squared_error: 0.0613\n",
            "Epoch 105/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1051 - mean_squared_error: 0.1051\n",
            "Epoch 105: val_mean_squared_error did not improve from 0.05962\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1048 - mean_squared_error: 0.1048 - val_loss: 0.0611 - val_mean_squared_error: 0.0611\n",
            "Model: \"sequential_70\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_383 (Dense)           (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_384 (Dense)           (None, 8)                 264       \n",
            "                                                                 \n",
            " dense_385 (Dense)           (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_386 (Dense)           (None, 4)                 36        \n",
            "                                                                 \n",
            " dense_387 (Dense)           (None, 2)                 10        \n",
            "                                                                 \n",
            " dense_388 (Dense)           (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_389 (Dense)           (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_390 (Dense)           (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,453\n",
            "Trainable params: 1,453\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "dict_keys(['loss', 'mean_squared_error', 'val_loss', 'val_mean_squared_error'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# > Model Dlat\n",
        "\n"
      ],
      "metadata": {
        "id": "rt-07cTThBin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Building a model with SimpleRNN \n",
        "# Building a model with SimpleRNN \n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from tensorflow.keras.layers import (\n",
        "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
        ")\n",
        "#define model and model parameters\n",
        "\n",
        "model_lat = Sequential()\n",
        "n_cols=df_lat_trx.shape[1]\n",
        "\n",
        "model_lat.add(Dense(units=32, input_shape=(n_cols,), activation=layer1)) \n",
        "model_lat.add(Dense(16, activation=layer1))\n",
        "model_lat.add(Dense(4, activation=layer1))\n",
        "model_lat.add(Dense(2, activation=layer1)) \n",
        "model_lat.add(Dense(2, activation=layer1))\n",
        "model_lat.add(Dense(2, activation=layer1))\n",
        "model_lat.add(Dense(1))\n",
        "\n",
        "\n",
        "#compile model\n",
        "model_lat.compile(loss='mse', optimizer='adam',metrics=['mean_squared_error']) \n",
        "#fit model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpointer = ModelCheckpoint(filepath='lat_weights.hdf5', \n",
        "                               save_best_only=True, save_weights_only=True, \n",
        "                               monitor='val_mean_squared_error', mode='min', \n",
        "                               verbose=1)\n",
        "#set early stopping monitor so the model stops training when it won't improve anymore\n",
        "early_stopping_monitor = EarlyStopping(patience=patience)\n",
        "#train model\n",
        "history=model_lat.fit(df_lat_trx, df_lat_try, validation_data=(df_lat_testx,df_lat_testy), epochs=epochs, \n",
        "                  callbacks=[early_stopping_monitor,checkpointer], batch_size=batch_size, verbose=1,shuffle=True)\n",
        "model_lat.summary()\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "#mean_squared_error: 0.0226 me ola\n",
        "#mean_squared_error: 0.0080  xwris batch normalisation\n",
        "\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['mean_squared_error'])\n",
        "plt.plot(history.history['val_mean_squared_error'])\n",
        "plt.title('model mean_squared_error')\n",
        "plt.ylabel('mean_squared_error')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mBz1pSu_hFLS",
        "outputId": "73481bae-329e-425a-df9d-d0ab973871e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000\n",
            "253/256 [============================>.] - ETA: 0s - loss: 0.6763 - mean_squared_error: 0.6763\n",
            "Epoch 1: val_mean_squared_error improved from inf to 0.50359, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 3s 4ms/step - loss: 0.6732 - mean_squared_error: 0.6732 - val_loss: 0.5036 - val_mean_squared_error: 0.5036\n",
            "Epoch 2/10000\n",
            "245/256 [===========================>..] - ETA: 0s - loss: 0.4627 - mean_squared_error: 0.4627\n",
            "Epoch 2: val_mean_squared_error improved from 0.50359 to 0.37394, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.4697 - mean_squared_error: 0.4697 - val_loss: 0.3739 - val_mean_squared_error: 0.3739\n",
            "Epoch 3/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.3682 - mean_squared_error: 0.3682\n",
            "Epoch 3: val_mean_squared_error improved from 0.37394 to 0.27739, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.2774 - val_mean_squared_error: 0.2774\n",
            "Epoch 4/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.2924 - mean_squared_error: 0.2924\n",
            "Epoch 4: val_mean_squared_error improved from 0.27739 to 0.23114, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2852 - mean_squared_error: 0.2852 - val_loss: 0.2311 - val_mean_squared_error: 0.2311\n",
            "Epoch 5/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.2513 - mean_squared_error: 0.2513\n",
            "Epoch 5: val_mean_squared_error improved from 0.23114 to 0.19802, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2463 - mean_squared_error: 0.2463 - val_loss: 0.1980 - val_mean_squared_error: 0.1980\n",
            "Epoch 6/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1998 - mean_squared_error: 0.1998\n",
            "Epoch 6: val_mean_squared_error improved from 0.19802 to 0.18409, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2233 - mean_squared_error: 0.2233 - val_loss: 0.1841 - val_mean_squared_error: 0.1841\n",
            "Epoch 7/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.2161 - mean_squared_error: 0.2161\n",
            "Epoch 7: val_mean_squared_error improved from 0.18409 to 0.16366, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.2081 - mean_squared_error: 0.2081 - val_loss: 0.1637 - val_mean_squared_error: 0.1637\n",
            "Epoch 8/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1894 - mean_squared_error: 0.1894\n",
            "Epoch 8: val_mean_squared_error improved from 0.16366 to 0.15649, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1961 - mean_squared_error: 0.1961 - val_loss: 0.1565 - val_mean_squared_error: 0.1565\n",
            "Epoch 9/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1703 - mean_squared_error: 0.1703\n",
            "Epoch 9: val_mean_squared_error improved from 0.15649 to 0.14828, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1858 - mean_squared_error: 0.1858 - val_loss: 0.1483 - val_mean_squared_error: 0.1483\n",
            "Epoch 10/10000\n",
            "245/256 [===========================>..] - ETA: 0s - loss: 0.1821 - mean_squared_error: 0.1821\n",
            "Epoch 10: val_mean_squared_error improved from 0.14828 to 0.14407, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1790 - mean_squared_error: 0.1790 - val_loss: 0.1441 - val_mean_squared_error: 0.1441\n",
            "Epoch 11/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1758 - mean_squared_error: 0.1758\n",
            "Epoch 11: val_mean_squared_error improved from 0.14407 to 0.13811, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1735 - mean_squared_error: 0.1735 - val_loss: 0.1381 - val_mean_squared_error: 0.1381\n",
            "Epoch 12/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1646 - mean_squared_error: 0.1646\n",
            "Epoch 12: val_mean_squared_error did not improve from 0.13811\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1685 - mean_squared_error: 0.1685 - val_loss: 0.1381 - val_mean_squared_error: 0.1381\n",
            "Epoch 13/10000\n",
            "246/256 [===========================>..] - ETA: 0s - loss: 0.1564 - mean_squared_error: 0.1564\n",
            "Epoch 13: val_mean_squared_error improved from 0.13811 to 0.13233, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1642 - mean_squared_error: 0.1642 - val_loss: 0.1323 - val_mean_squared_error: 0.1323\n",
            "Epoch 14/10000\n",
            "249/256 [============================>.] - ETA: 0s - loss: 0.1638 - mean_squared_error: 0.1638\n",
            "Epoch 14: val_mean_squared_error did not improve from 0.13233\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1616 - mean_squared_error: 0.1616 - val_loss: 0.1347 - val_mean_squared_error: 0.1347\n",
            "Epoch 15/10000\n",
            "244/256 [===========================>..] - ETA: 0s - loss: 0.1635 - mean_squared_error: 0.1635\n",
            "Epoch 15: val_mean_squared_error improved from 0.13233 to 0.12849, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1591 - mean_squared_error: 0.1591 - val_loss: 0.1285 - val_mean_squared_error: 0.1285\n",
            "Epoch 16/10000\n",
            "249/256 [============================>.] - ETA: 0s - loss: 0.1572 - mean_squared_error: 0.1572\n",
            "Epoch 16: val_mean_squared_error improved from 0.12849 to 0.12718, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1559 - mean_squared_error: 0.1559 - val_loss: 0.1272 - val_mean_squared_error: 0.1272\n",
            "Epoch 17/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1413 - mean_squared_error: 0.1413\n",
            "Epoch 17: val_mean_squared_error improved from 0.12718 to 0.12485, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1531 - mean_squared_error: 0.1531 - val_loss: 0.1249 - val_mean_squared_error: 0.1249\n",
            "Epoch 18/10000\n",
            "235/256 [==========================>...] - ETA: 0s - loss: 0.1555 - mean_squared_error: 0.1555\n",
            "Epoch 18: val_mean_squared_error did not improve from 0.12485\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1513 - mean_squared_error: 0.1513 - val_loss: 0.1276 - val_mean_squared_error: 0.1276\n",
            "Epoch 19/10000\n",
            "245/256 [===========================>..] - ETA: 0s - loss: 0.1516 - mean_squared_error: 0.1516\n",
            "Epoch 19: val_mean_squared_error improved from 0.12485 to 0.12244, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1485 - mean_squared_error: 0.1485 - val_loss: 0.1224 - val_mean_squared_error: 0.1224\n",
            "Epoch 20/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.1509 - mean_squared_error: 0.1509\n",
            "Epoch 20: val_mean_squared_error did not improve from 0.12244\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1470 - mean_squared_error: 0.1470 - val_loss: 0.1225 - val_mean_squared_error: 0.1225\n",
            "Epoch 21/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.1394 - mean_squared_error: 0.1394\n",
            "Epoch 21: val_mean_squared_error did not improve from 0.12244\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1459 - mean_squared_error: 0.1459 - val_loss: 0.1246 - val_mean_squared_error: 0.1246\n",
            "Epoch 22/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1480 - mean_squared_error: 0.1480\n",
            "Epoch 22: val_mean_squared_error improved from 0.12244 to 0.11608, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1431 - mean_squared_error: 0.1431 - val_loss: 0.1161 - val_mean_squared_error: 0.1161\n",
            "Epoch 23/10000\n",
            "240/256 [===========================>..] - ETA: 0s - loss: 0.1481 - mean_squared_error: 0.1481\n",
            "Epoch 23: val_mean_squared_error improved from 0.11608 to 0.11509, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1435 - mean_squared_error: 0.1435 - val_loss: 0.1151 - val_mean_squared_error: 0.1151\n",
            "Epoch 24/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.1233 - mean_squared_error: 0.1233\n",
            "Epoch 24: val_mean_squared_error improved from 0.11509 to 0.11259, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1401 - mean_squared_error: 0.1401 - val_loss: 0.1126 - val_mean_squared_error: 0.1126\n",
            "Epoch 25/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1409 - mean_squared_error: 0.1409\n",
            "Epoch 25: val_mean_squared_error did not improve from 0.11259\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1384 - mean_squared_error: 0.1384 - val_loss: 0.1184 - val_mean_squared_error: 0.1184\n",
            "Epoch 26/10000\n",
            "233/256 [==========================>...] - ETA: 0s - loss: 0.1444 - mean_squared_error: 0.1444\n",
            "Epoch 26: val_mean_squared_error did not improve from 0.11259\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1380 - mean_squared_error: 0.1380 - val_loss: 0.1174 - val_mean_squared_error: 0.1174\n",
            "Epoch 27/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1411 - mean_squared_error: 0.1411\n",
            "Epoch 27: val_mean_squared_error did not improve from 0.11259\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1361 - mean_squared_error: 0.1361 - val_loss: 0.1166 - val_mean_squared_error: 0.1166\n",
            "Epoch 28/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.1372 - mean_squared_error: 0.1372\n",
            "Epoch 28: val_mean_squared_error did not improve from 0.11259\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1342 - mean_squared_error: 0.1342 - val_loss: 0.1139 - val_mean_squared_error: 0.1139\n",
            "Epoch 29/10000\n",
            "236/256 [==========================>...] - ETA: 0s - loss: 0.1385 - mean_squared_error: 0.1385\n",
            "Epoch 29: val_mean_squared_error did not improve from 0.11259\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1336 - mean_squared_error: 0.1336 - val_loss: 0.1146 - val_mean_squared_error: 0.1146\n",
            "Epoch 30/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.1322 - mean_squared_error: 0.1322\n",
            "Epoch 30: val_mean_squared_error improved from 0.11259 to 0.11187, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1348 - mean_squared_error: 0.1348 - val_loss: 0.1119 - val_mean_squared_error: 0.1119\n",
            "Epoch 31/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1224 - mean_squared_error: 0.1224\n",
            "Epoch 31: val_mean_squared_error improved from 0.11187 to 0.11115, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1317 - mean_squared_error: 0.1317 - val_loss: 0.1111 - val_mean_squared_error: 0.1111\n",
            "Epoch 32/10000\n",
            "244/256 [===========================>..] - ETA: 0s - loss: 0.1327 - mean_squared_error: 0.1327\n",
            "Epoch 32: val_mean_squared_error improved from 0.11115 to 0.11057, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1309 - mean_squared_error: 0.1309 - val_loss: 0.1106 - val_mean_squared_error: 0.1106\n",
            "Epoch 33/10000\n",
            "247/256 [===========================>..] - ETA: 0s - loss: 0.1316 - mean_squared_error: 0.1316\n",
            "Epoch 33: val_mean_squared_error did not improve from 0.11057\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1293 - mean_squared_error: 0.1293 - val_loss: 0.1106 - val_mean_squared_error: 0.1106\n",
            "Epoch 34/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1286 - mean_squared_error: 0.1286\n",
            "Epoch 34: val_mean_squared_error did not improve from 0.11057\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1287 - mean_squared_error: 0.1287 - val_loss: 0.1112 - val_mean_squared_error: 0.1112\n",
            "Epoch 35/10000\n",
            "244/256 [===========================>..] - ETA: 0s - loss: 0.1305 - mean_squared_error: 0.1305\n",
            "Epoch 35: val_mean_squared_error improved from 0.11057 to 0.10831, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1275 - mean_squared_error: 0.1275 - val_loss: 0.1083 - val_mean_squared_error: 0.1083\n",
            "Epoch 36/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.1290 - mean_squared_error: 0.1290\n",
            "Epoch 36: val_mean_squared_error did not improve from 0.10831\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1263 - mean_squared_error: 0.1263 - val_loss: 0.1137 - val_mean_squared_error: 0.1137\n",
            "Epoch 37/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1283 - mean_squared_error: 0.1283\n",
            "Epoch 37: val_mean_squared_error improved from 0.10831 to 0.10819, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1258 - mean_squared_error: 0.1258 - val_loss: 0.1082 - val_mean_squared_error: 0.1082\n",
            "Epoch 38/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.1288 - mean_squared_error: 0.1288\n",
            "Epoch 38: val_mean_squared_error did not improve from 0.10819\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1247 - mean_squared_error: 0.1247 - val_loss: 0.1100 - val_mean_squared_error: 0.1100\n",
            "Epoch 39/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.1270 - mean_squared_error: 0.1270\n",
            "Epoch 39: val_mean_squared_error did not improve from 0.10819\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1242 - mean_squared_error: 0.1242 - val_loss: 0.1125 - val_mean_squared_error: 0.1125\n",
            "Epoch 40/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1266 - mean_squared_error: 0.1266\n",
            "Epoch 40: val_mean_squared_error improved from 0.10819 to 0.10685, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1234 - mean_squared_error: 0.1234 - val_loss: 0.1069 - val_mean_squared_error: 0.1069\n",
            "Epoch 41/10000\n",
            "244/256 [===========================>..] - ETA: 0s - loss: 0.1116 - mean_squared_error: 0.1116\n",
            "Epoch 41: val_mean_squared_error improved from 0.10685 to 0.10653, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1221 - mean_squared_error: 0.1221 - val_loss: 0.1065 - val_mean_squared_error: 0.1065\n",
            "Epoch 42/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.1163 - mean_squared_error: 0.1163\n",
            "Epoch 42: val_mean_squared_error did not improve from 0.10653\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1223 - mean_squared_error: 0.1223 - val_loss: 0.1118 - val_mean_squared_error: 0.1118\n",
            "Epoch 43/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1259 - mean_squared_error: 0.1259\n",
            "Epoch 43: val_mean_squared_error did not improve from 0.10653\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1213 - mean_squared_error: 0.1213 - val_loss: 0.1081 - val_mean_squared_error: 0.1081\n",
            "Epoch 44/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.1176 - mean_squared_error: 0.1176\n",
            "Epoch 44: val_mean_squared_error did not improve from 0.10653\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1197 - mean_squared_error: 0.1197 - val_loss: 0.1067 - val_mean_squared_error: 0.1067\n",
            "Epoch 45/10000\n",
            "246/256 [===========================>..] - ETA: 0s - loss: 0.1107 - mean_squared_error: 0.1107\n",
            "Epoch 45: val_mean_squared_error improved from 0.10653 to 0.10432, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1198 - mean_squared_error: 0.1198 - val_loss: 0.1043 - val_mean_squared_error: 0.1043\n",
            "Epoch 46/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.1246 - mean_squared_error: 0.1246\n",
            "Epoch 46: val_mean_squared_error did not improve from 0.10432\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1209 - mean_squared_error: 0.1209 - val_loss: 0.1062 - val_mean_squared_error: 0.1062\n",
            "Epoch 47/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.1157 - mean_squared_error: 0.1157\n",
            "Epoch 47: val_mean_squared_error did not improve from 0.10432\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1171 - mean_squared_error: 0.1171 - val_loss: 0.1144 - val_mean_squared_error: 0.1144\n",
            "Epoch 48/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1041 - mean_squared_error: 0.1041\n",
            "Epoch 48: val_mean_squared_error did not improve from 0.10432\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1176 - mean_squared_error: 0.1176 - val_loss: 0.1095 - val_mean_squared_error: 0.1095\n",
            "Epoch 49/10000\n",
            "246/256 [===========================>..] - ETA: 0s - loss: 0.1193 - mean_squared_error: 0.1193\n",
            "Epoch 49: val_mean_squared_error did not improve from 0.10432\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1169 - mean_squared_error: 0.1169 - val_loss: 0.1075 - val_mean_squared_error: 0.1075\n",
            "Epoch 50/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.1073 - mean_squared_error: 0.1073\n",
            "Epoch 50: val_mean_squared_error did not improve from 0.10432\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1158 - mean_squared_error: 0.1158 - val_loss: 0.1089 - val_mean_squared_error: 0.1089\n",
            "Epoch 51/10000\n",
            "234/256 [==========================>...] - ETA: 0s - loss: 0.1206 - mean_squared_error: 0.1206\n",
            "Epoch 51: val_mean_squared_error did not improve from 0.10432\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1150 - mean_squared_error: 0.1150 - val_loss: 0.1059 - val_mean_squared_error: 0.1059\n",
            "Epoch 52/10000\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.1151 - mean_squared_error: 0.1151\n",
            "Epoch 52: val_mean_squared_error improved from 0.10432 to 0.10272, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1151 - mean_squared_error: 0.1151 - val_loss: 0.1027 - val_mean_squared_error: 0.1027\n",
            "Epoch 53/10000\n",
            "237/256 [==========================>...] - ETA: 0s - loss: 0.1180 - mean_squared_error: 0.1180\n",
            "Epoch 53: val_mean_squared_error improved from 0.10272 to 0.10134, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1140 - mean_squared_error: 0.1140 - val_loss: 0.1013 - val_mean_squared_error: 0.1013\n",
            "Epoch 54/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1146 - mean_squared_error: 0.1146\n",
            "Epoch 54: val_mean_squared_error did not improve from 0.10134\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1126 - mean_squared_error: 0.1126 - val_loss: 0.1030 - val_mean_squared_error: 0.1030\n",
            "Epoch 55/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.1147 - mean_squared_error: 0.1147\n",
            "Epoch 55: val_mean_squared_error did not improve from 0.10134\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1120 - mean_squared_error: 0.1120 - val_loss: 0.1048 - val_mean_squared_error: 0.1048\n",
            "Epoch 56/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.1052 - mean_squared_error: 0.1052\n",
            "Epoch 56: val_mean_squared_error did not improve from 0.10134\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1109 - mean_squared_error: 0.1109 - val_loss: 0.1033 - val_mean_squared_error: 0.1033\n",
            "Epoch 57/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.1117 - mean_squared_error: 0.1117\n",
            "Epoch 57: val_mean_squared_error improved from 0.10134 to 0.10033, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1104 - mean_squared_error: 0.1104 - val_loss: 0.1003 - val_mean_squared_error: 0.1003\n",
            "Epoch 58/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1042 - mean_squared_error: 0.1042\n",
            "Epoch 58: val_mean_squared_error did not improve from 0.10033\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1105 - mean_squared_error: 0.1105 - val_loss: 0.1096 - val_mean_squared_error: 0.1096\n",
            "Epoch 59/10000\n",
            "246/256 [===========================>..] - ETA: 0s - loss: 0.1115 - mean_squared_error: 0.1115\n",
            "Epoch 59: val_mean_squared_error did not improve from 0.10033\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1094 - mean_squared_error: 0.1094 - val_loss: 0.1017 - val_mean_squared_error: 0.1017\n",
            "Epoch 60/10000\n",
            "246/256 [===========================>..] - ETA: 0s - loss: 0.1109 - mean_squared_error: 0.1109\n",
            "Epoch 60: val_mean_squared_error did not improve from 0.10033\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1108 - mean_squared_error: 0.1108 - val_loss: 0.1023 - val_mean_squared_error: 0.1023\n",
            "Epoch 61/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1098 - mean_squared_error: 0.1098\n",
            "Epoch 61: val_mean_squared_error did not improve from 0.10033\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1079 - mean_squared_error: 0.1079 - val_loss: 0.1085 - val_mean_squared_error: 0.1085\n",
            "Epoch 62/10000\n",
            "241/256 [===========================>..] - ETA: 0s - loss: 0.1104 - mean_squared_error: 0.1104\n",
            "Epoch 62: val_mean_squared_error did not improve from 0.10033\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1080 - mean_squared_error: 0.1080 - val_loss: 0.1008 - val_mean_squared_error: 0.1008\n",
            "Epoch 63/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.1103 - mean_squared_error: 0.1103\n",
            "Epoch 63: val_mean_squared_error did not improve from 0.10033\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1073 - mean_squared_error: 0.1073 - val_loss: 0.1069 - val_mean_squared_error: 0.1069\n",
            "Epoch 64/10000\n",
            "236/256 [==========================>...] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.1015\n",
            "Epoch 64: val_mean_squared_error did not improve from 0.10033\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1061 - mean_squared_error: 0.1061 - val_loss: 0.1045 - val_mean_squared_error: 0.1045\n",
            "Epoch 65/10000\n",
            "255/256 [============================>.] - ETA: 0s - loss: 0.1064 - mean_squared_error: 0.1064\n",
            "Epoch 65: val_mean_squared_error improved from 0.10033 to 0.09776, saving model to lat_weights.hdf5\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1063 - mean_squared_error: 0.1063 - val_loss: 0.0978 - val_mean_squared_error: 0.0978\n",
            "Epoch 66/10000\n",
            "250/256 [============================>.] - ETA: 0s - loss: 0.1055 - mean_squared_error: 0.1055\n",
            "Epoch 66: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1048 - mean_squared_error: 0.1048 - val_loss: 0.1036 - val_mean_squared_error: 0.1036\n",
            "Epoch 67/10000\n",
            "254/256 [============================>.] - ETA: 0s - loss: 0.1057 - mean_squared_error: 0.1057\n",
            "Epoch 67: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1053 - mean_squared_error: 0.1053 - val_loss: 0.1017 - val_mean_squared_error: 0.1017\n",
            "Epoch 68/10000\n",
            "248/256 [============================>.] - ETA: 0s - loss: 0.1040 - mean_squared_error: 0.1040\n",
            "Epoch 68: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1040 - mean_squared_error: 0.1040 - val_loss: 0.1023 - val_mean_squared_error: 0.1023\n",
            "Epoch 69/10000\n",
            "244/256 [===========================>..] - ETA: 0s - loss: 0.1063 - mean_squared_error: 0.1063\n",
            "Epoch 69: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1039 - mean_squared_error: 0.1039 - val_loss: 0.1050 - val_mean_squared_error: 0.1050\n",
            "Epoch 70/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.0992 - mean_squared_error: 0.0992\n",
            "Epoch 70: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1040 - mean_squared_error: 0.1040 - val_loss: 0.1019 - val_mean_squared_error: 0.1019\n",
            "Epoch 71/10000\n",
            "244/256 [===========================>..] - ETA: 0s - loss: 0.0992 - mean_squared_error: 0.0992\n",
            "Epoch 71: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1038 - mean_squared_error: 0.1038 - val_loss: 0.1090 - val_mean_squared_error: 0.1090\n",
            "Epoch 72/10000\n",
            "242/256 [===========================>..] - ETA: 0s - loss: 0.1045 - mean_squared_error: 0.1045\n",
            "Epoch 72: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1016 - mean_squared_error: 0.1016 - val_loss: 0.1048 - val_mean_squared_error: 0.1048\n",
            "Epoch 73/10000\n",
            "248/256 [============================>.] - ETA: 0s - loss: 0.1016 - mean_squared_error: 0.1016\n",
            "Epoch 73: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1015 - mean_squared_error: 0.1015 - val_loss: 0.1001 - val_mean_squared_error: 0.1001\n",
            "Epoch 74/10000\n",
            "236/256 [==========================>...] - ETA: 0s - loss: 0.1030 - mean_squared_error: 0.1030\n",
            "Epoch 74: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1005 - mean_squared_error: 0.1005 - val_loss: 0.1038 - val_mean_squared_error: 0.1038\n",
            "Epoch 75/10000\n",
            "239/256 [===========================>..] - ETA: 0s - loss: 0.1043 - mean_squared_error: 0.1043\n",
            "Epoch 75: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1015 - mean_squared_error: 0.1015 - val_loss: 0.1027 - val_mean_squared_error: 0.1027\n",
            "Epoch 76/10000\n",
            "238/256 [==========================>...] - ETA: 0s - loss: 0.0945 - mean_squared_error: 0.0945\n",
            "Epoch 76: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.0999 - mean_squared_error: 0.0999 - val_loss: 0.1060 - val_mean_squared_error: 0.1060\n",
            "Epoch 77/10000\n",
            "244/256 [===========================>..] - ETA: 0s - loss: 0.1007 - mean_squared_error: 0.1007\n",
            "Epoch 77: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.0993 - mean_squared_error: 0.0993 - val_loss: 0.1042 - val_mean_squared_error: 0.1042\n",
            "Epoch 78/10000\n",
            "243/256 [===========================>..] - ETA: 0s - loss: 0.0952 - mean_squared_error: 0.0952\n",
            "Epoch 78: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.1006 - mean_squared_error: 0.1006 - val_loss: 0.1036 - val_mean_squared_error: 0.1036\n",
            "Epoch 79/10000\n",
            "245/256 [===========================>..] - ETA: 0s - loss: 0.0997 - mean_squared_error: 0.0997\n",
            "Epoch 79: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.0978 - mean_squared_error: 0.0978 - val_loss: 0.1088 - val_mean_squared_error: 0.1088\n",
            "Epoch 80/10000\n",
            "245/256 [===========================>..] - ETA: 0s - loss: 0.0927 - mean_squared_error: 0.0927\n",
            "Epoch 80: val_mean_squared_error did not improve from 0.09776\n",
            "256/256 [==============================] - 1s 3ms/step - loss: 0.0988 - mean_squared_error: 0.0988 - val_loss: 0.1063 - val_mean_squared_error: 0.1063\n",
            "Model: \"sequential_71\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_391 (Dense)           (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_392 (Dense)           (None, 16)                528       \n",
            "                                                                 \n",
            " dense_393 (Dense)           (None, 4)                 68        \n",
            "                                                                 \n",
            " dense_394 (Dense)           (None, 2)                 10        \n",
            "                                                                 \n",
            " dense_395 (Dense)           (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_396 (Dense)           (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_397 (Dense)           (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,677\n",
            "Trainable params: 1,677\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "dict_keys(['loss', 'mean_squared_error', 'val_loss', 'val_mean_squared_error'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# > Data creation, Cross Reference\n",
        "\n"
      ],
      "metadata": {
        "id": "OGL-0Br5hLiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainPredictlat = model_lat.predict(df_lat_trx, batch_size=batch_size)\n",
        "testPredictlat = model_lat.predict(df_lat_valx, batch_size=batch_size)\n",
        "trainPredictlon = model_lon.predict(df_lon_trx, batch_size=batch_size)\n",
        "testPredictlon = model_lon.predict(df_lon_valx, batch_size=batch_size)\n",
        "\n",
        "\n",
        "trainPredictlat = np.reshape(trainPredictlat, (-1, 1))\n",
        "ytr1dlat = np.reshape(df_lat_try, (-1, 1))\n",
        "\n",
        "testPredictlat = np.reshape(testPredictlat, (-1, 1))\n",
        "yte1dlat = np.reshape(df_lat_valy, (-1, 1))\n",
        "\n",
        "trainPredictlon = np.reshape(trainPredictlon, (-1, 1))\n",
        "ytr1dlon = np.reshape(df_lon_try, (-1, 1))\n",
        "\n",
        "testPredictlon = np.reshape(testPredictlon, (-1, 1))\n",
        "yte1dlon = np.reshape(df_lon_valy, (-1, 1))\n",
        "\n",
        "\n",
        "trainPredictlat = scaler8.inverse_transform(trainPredictlat)\n",
        "trainYlat = scaler8.inverse_transform(ytr1dlat)\n",
        "\n",
        "trainPredictlon = scaler2.inverse_transform(trainPredictlon)\n",
        "trainYlon = scaler2.inverse_transform(ytr1dlon)\n",
        "\n",
        "testPredictlat = scaler12.inverse_transform(testPredictlat)\n",
        "testYlat = scaler12.inverse_transform(yte1dlat)\n",
        "\n",
        "testPredictlon = scaler6.inverse_transform(testPredictlon)\n",
        "testYlon = scaler6.inverse_transform(yte1dlon)\n",
        "\n",
        "\n",
        "trainPredictlon=pd.DataFrame(trainPredictlon,columns=['dlon'])\n",
        "trainPredictlat=pd.DataFrame(trainPredictlat,columns=['dlat'])\n",
        "testPredictlon =pd.DataFrame(testPredictlon,columns=['dlon'])\n",
        "testPredictlat =pd.DataFrame(testPredictlat,columns=['dlat'])\n",
        "\n",
        "\n",
        "trainYlon=pd.DataFrame(trainYlon,columns=['dlon'])\n",
        "trainYlat=pd.DataFrame(trainYlat,columns=['dlat'])\n",
        "testYlon =pd.DataFrame(testYlon,columns=['dlon'])\n",
        "testYlat =pd.DataFrame(testYlat,columns=['dlat'])\n",
        "\n",
        "\n",
        "dfpredictdlon= pd.DataFrame([])\n",
        "dftestdlon= pd.DataFrame([])\n",
        "trainPreddlon =pd.DataFrame([])\n",
        "trainRealdlon = pd.DataFrame([])\n",
        "dfpredictdlat= pd.DataFrame([])\n",
        "dftestdlat= pd.DataFrame([])\n",
        "trainPreddlat = pd.DataFrame([])\n",
        "trainRealdlat =pd.DataFrame([])\n",
        "\n",
        "\n",
        "dfpredictdlon['dlon']=testPredictlon['dlon']\n",
        "trainPreddlat['dlat']=trainPredictlat['dlat']\n",
        "dftestdlon['dlon']= testYlon['dlon']\n",
        "dftestdlat['dlat']= testYlat['dlat']\n",
        "trainPreddlon['dlon'] = trainPredictlon['dlon']\n",
        "dfpredictdlat['dlat']= testPredictlat['dlat']\n",
        "trainRealdlon['dlon'] = trainYlon['dlon']\n",
        "trainRealdlat['dlat'] = trainYlat['dlat']\n",
        "\n",
        "dfpredictdlon['lon']=dfvalzlon[\"lon(t-1)\"].values\n",
        "dfpredictdlat['lat'] =dfvalzlat[\"lat(t-1)\"].values\n",
        "dftestdlon['lon']=dfvalzlon[\"lon(t-1)\"].values\n",
        "dftestdlat['lat'] =dfvalzlat[\"lat(t-1)\"].values\n",
        "trainPreddlon['lon'] =dftrainzlon[\"lon(t-1)\"].values\n",
        "trainRealdlon['lon'] =dftrainzlon[\"lon(t-1)\"].values\n",
        "trainPreddlat['lat'] =dftrainzlat[\"lat(t-1)\"].values\n",
        "trainRealdlat['lat'] =dftrainzlat[\"lat(t-1)\"].values\n",
        "\n",
        "trainPredlon= pd.DataFrame([])\n",
        "trainPredlat= pd.DataFrame([])\n",
        "trainReallon= pd.DataFrame([])\n",
        "trainReallat= pd.DataFrame([])\n",
        "dfpredictlon= pd.DataFrame([])\n",
        "dfpredictlat= pd.DataFrame([])\n",
        "dftestlon= pd.DataFrame([])\n",
        "dftestlat= pd.DataFrame([])\n",
        "dfpredictdlon['flon']=dfpredictdlon.sum(axis = 1, skipna = True)\n",
        "dfpredictdlat['flat']= dfpredictdlat.sum(axis = 1, skipna = True)\n",
        "dftestdlon['flon']= dftestdlon.sum(axis = 1, skipna = True)\n",
        "dftestdlat['flat']= dftestdlat.sum(axis = 1, skipna = True)\n",
        "\n",
        "trainPreddlon['flon']= trainPreddlon.sum(axis = 1, skipna = True)\n",
        "trainPreddlat['flat']= trainPreddlat.sum(axis = 1, skipna = True)\n",
        "trainRealdlon['flon']= trainRealdlon.sum(axis = 1, skipna = True)\n",
        "trainRealdlat['flat']= trainRealdlat.sum(axis = 1, skipna = True)\n",
        "\n",
        "trainPredlon['lon']= trainPreddlon['flon']\n",
        "trainPredlat['lat']= trainPreddlat['flat']\n",
        "trainReallon['lon']= trainRealdlon['flon']\n",
        "trainReallat['lat']= trainRealdlat['flat']\n",
        "dfpredictlon['lon']=dfpredictdlon['flon']\n",
        "dfpredictlat['lat']=dfpredictdlat['flat']\n",
        "dftestlon['lon']=dftestdlon['flon']\n",
        "dftestlat['lat']=dftestdlat['flat']\n",
        "\n",
        "\n",
        "\n",
        "df_predtrainresults= pd.DataFrame([])\n",
        "df_trainresults = pd.DataFrame([])\n",
        "df1= pd.DataFrame([])\n",
        "\n",
        "df_predtrainresults[\"long_latPREDICT\"] = list(zip(trainPredlon['lon'], trainPredlat['lat']))\n",
        "df_trainresults[\"long_latRESULT\"] = list(zip(trainReallon['lon'], trainReallat['lat']))\n",
        "\n",
        "df1['prediction']=df_predtrainresults[\"long_latPREDICT\"]\n",
        "df1['reality']=df_trainresults[\"long_latRESULT\"]\n",
        "import math\n",
        "def distance(df1):\n",
        "  return math.dist(df1['prediction'],df1['reality'])\n",
        "\n",
        "df1[\"distance(m)\"] = df1.apply(distance, axis =1)\n",
        "df1[\"distance(m)\"].mean()\n",
        "\n",
        "\n",
        "df_predresults = pd.DataFrame([])\n",
        "df_testresults = pd.DataFrame([])\n",
        "df= pd.DataFrame([])\n",
        "df_predresults[\"long_latPREDICT\"] = list(zip(dfpredictlon['lon'], dfpredictlat['lat']))\n",
        "df_testresults[\"long_latRESULT\"] = list(zip(dftestlon['lon'], dftestlat['lat']))\n",
        "\n",
        "\n",
        "\n",
        "df['prediction']=df_predresults[\"long_latPREDICT\"]\n",
        "df['reality']=df_testresults[\"long_latRESULT\"]\n",
        "\n",
        "\n",
        "def distance(df):\n",
        "  return math.dist(df['prediction'],df['reality'])\n",
        "\n",
        "df[\"distance(m)\"] = df.apply(distance, axis =1)\n",
        "df[\"distance(m)\"].mean()\n",
        "\n",
        "\n",
        "df['dt']=df_valx[\"dt(t)\"].values\n",
        "\n",
        "\n",
        "\n",
        "index1=0\n",
        "index2=0\n",
        "index3=0\n",
        "index4=0\n",
        "index5=0\n",
        "index6=0\n",
        "index7=0\n",
        "index8=0\n",
        "index9=0\n",
        "index10=0\n",
        "index11=0\n",
        "index12=0\n",
        "index13=0\n",
        "index14=0\n",
        "index15=0\n",
        "index16=0\n",
        "index17=0\n",
        "index18=0\n",
        "index19=0\n",
        "index20=0\n",
        "index21=0\n",
        "index22=0\n",
        "index23=0\n",
        "index24=0\n",
        "index25=0\n",
        "index26=0\n",
        "index27=0\n",
        "index28=0\n",
        "index29=0\n",
        "dist1740=0\n",
        "dist1680=0\n",
        "dist1620=0\n",
        "dist1560=0\n",
        "dist1500=0\n",
        "dist1440=0\n",
        "dist1380=0\n",
        "dist1320=0\n",
        "dist1260=0\n",
        "dist1200=0\n",
        "dist1140=0\n",
        "dist1080=0\n",
        "dist1020=0\n",
        "dist960=0\n",
        "dist900=0\n",
        "dist840=0\n",
        "dist780=0\n",
        "dist720=0\n",
        "dist660=0\n",
        "dist600=0\n",
        "dist540=0\n",
        "dist480=0\n",
        "dist420=0\n",
        "dist360=0\n",
        "dist300=0\n",
        "dist240=0\n",
        "dist180=0\n",
        "dist120=0\n",
        "dist60=0\n",
        "for index, row in df.iterrows(): \n",
        "    window= row[3]\n",
        "    if window>=1740 and window<=1800:\n",
        "            dist1740=dist1740+row[2] \n",
        "            index1=index1+1\n",
        "    if window>=1680 and window<1740:\n",
        "            dist1680=dist1680+row[2] \n",
        "            index2=index2+1\n",
        "    if window>=1620 and window<1680:\n",
        "            dist1620=dist1620+row[2] \n",
        "            index3=index3+1\n",
        "    if window>=1560 and window<1620:\n",
        "            dist1560=dist1560+row[2] \n",
        "            index4=index4+1\n",
        "    if window>=1500 and window<1560:\n",
        "            dist1500=dist1500+row[2] \n",
        "            index5=index5+1\n",
        "    if window>=1440 and window<1500:\n",
        "            dist1440=dist1440+row[2] \n",
        "            index6=index6+1\n",
        "    if window>=1380 and window<1440:\n",
        "            dist1380=dist1380+row[2] \n",
        "            index7=index7+1\n",
        "    if window>=1320 and window<1380:\n",
        "            dist1320=dist1320+row[2] \n",
        "            index8=index8+1\n",
        "    if window>=1260 and window<1320:\n",
        "            dist1260=dist1260+row[2] \n",
        "            index9=index9+1    \n",
        "    if window>=1200 and window<1260:\n",
        "            dist1200=dist1200+row[2] \n",
        "            index10=index10+1\n",
        "    if window>=1140 and window<1200:\n",
        "            dist1140=dist1140+row[2] \n",
        "            index11=index11+1\n",
        "    if window>=1080 and window<1140:\n",
        "            dist1080=dist1080+row[2] \n",
        "            index12=index12+1\n",
        "    if window>=1020 and window<1080:\n",
        "            dist1020=dist1020+row[2] \n",
        "            index13=index13+1\n",
        "    if window>=960 and window<1020:\n",
        "            dist960=dist960+row[2] \n",
        "            index14=index14+1\n",
        "    if window>=900 and window<960:\n",
        "            dist900=dist900+row[2] \n",
        "            index15=index15+1\n",
        "    if window>=840 and window<900:\n",
        "            dist840=dist840+row[2] \n",
        "            index16=index16+1\n",
        "    if window>=780 and window<840:\n",
        "            dist780=dist780+row[2] \n",
        "            index17=index17+1\n",
        "    if window>=720 and window<780:\n",
        "            dist720=dist720+row[2] \n",
        "            index18=index18+1\n",
        "    if window>=660 and window<720:\n",
        "            dist660=dist660+row[2] \n",
        "            index19=index19+1\n",
        "    if window>=600 and window<660:\n",
        "            dist600=dist600+row[2] \n",
        "            index20=index20+1\n",
        "    if window>=540 and window<600:\n",
        "            dist540=dist540+row[2] \n",
        "            index21=index21+1\n",
        "    if window>=480 and window<540:\n",
        "            dist480=dist480+row[2] \n",
        "            index22=index22+1\n",
        "    if window>=420 and window<480:\n",
        "            dist420=dist420+row[2] \n",
        "            index23=index23+1\n",
        "    if window>=360 and window<420:\n",
        "            dist360=dist360+row[2] \n",
        "            index24=index24+1\n",
        "    if window>=300 and window<360:\n",
        "            dist300=dist300+row[2] \n",
        "            index25=index25+1\n",
        "    if window>=240 and window<300:\n",
        "            dist240=dist240+row[2] \n",
        "            index26=index26+1\n",
        "    if window>=180 and window<240:\n",
        "            dist180=dist180+row[2] \n",
        "            index27=index27+1\n",
        "    if window>=120 and window<180:\n",
        "            dist120=dist120+row[2] \n",
        "            index28=index28+1\n",
        "    if window>=0 and window<120:\n",
        "            dist60=dist60+row[2] \n",
        "            index29=index29+1\n",
        "        \n",
        "#dist1740,dist1680,dist1620,dist1560,dist1500,dist1440,dist1380,dist1320,dist1200,dist1200,dist1140,\n",
        "#dist1080,dist1020,dist960,dist900,dist840,dist780,dist720,dist660,dist600,dist540,dist480,dist420,\n",
        "#dist360,dist300,dist240,dist180,dist120,dist60\n",
        "print(dist60)\n",
        "print( 'distance km under 2 minutes' ,(dist60/index29)/1000)\n",
        "print( 'distance km under 3 minutes' ,(dist120/index28)/1000)\n",
        "print( 'distance km under 4 minutes' ,(dist180/index27)/1000)  \n",
        "print( 'distance km under 5 minutes' ,(dist240/index26)/1000)  \n",
        "print( 'distance km under 6 minutes' ,(dist300/index25) /1000) \n",
        "print( 'distance km under 7 minutes' ,(dist360/index24) /1000) \n",
        "print( 'distance km under 8 minutes' ,(dist420/index23)/1000)\n",
        "#print( 'distance km under 9 minutes' ,(dist480/index22) /1000) \n",
        "print( 'distance km under 10 minutes' ,(dist540/index21) /1000) \n",
        "print( 'distance km under 11 minutes' ,(dist600/index20)/1000) \n",
        "print( 'distance km under 12 minutes' ,(dist660/index19) /1000) \n",
        "print( 'distance km under 13 minutes' ,(dist720/index18)/1000)\n",
        "print( 'distance km under 14 minutes' ,(dist780 /index17) /1000) \n",
        "print( 'distance km under 15 minutes' ,(dist840/index16) /1000) \n",
        "print( 'distance km under 16 minutes' ,(dist900/index15)/1000) \n",
        "#print( 'distance km under 17 minutes' ,(dist960/index14) /1000) \n",
        "#print( 'distance km under 18 minutes' ,(dist1020/index13/1000))\n",
        "print( 'distance km under 19 minutes' ,(dist1080/index12) /1000) \n",
        "print( 'distance km under 20 minutes' ,(dist1140/index11) /1000) \n",
        "#print( 'distance km under 21 minutes' ,(dist1200/index10)/1000) \n",
        "#print( 'distance km under 22 minutes' ,(dist1260/index9)  /1000)\n",
        "#print( 'distance km under 23 minutes' ,(dist1320 /index8)/1000)\n",
        "#print( 'distance km under 24 minutes' ,(dist1380/index7) /1000) \n",
        "#print( 'distance km under 25 minutes' ,(dist1440/index6)/1000)  \n",
        "#print( 'distance km under 26 minutes' ,(dist1500/index5)/1000) \n",
        "#print( 'distance km under 27 minutes' ,(dist1560/index4)/1000)  \n",
        "#print( 'distance km under 28 minutes' ,(dist1620/index3)/1000)\n",
        "print( 'distance km under 29 minutes' ,(dist1680/index2)/1000)  \n",
        "#print( 'distance km under 30 minutes' ,(dist1740/index1)/1000)  \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QrtXJNwhTRV",
        "outputId": "8c40f3ae-9807-4d7f-8433-21cd24a89021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256/256 [==============================] - 1s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "256/256 [==============================] - 1s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "1875.3785108964134\n",
            "distance km under 2 minutes 0.1704889555360376\n",
            "distance km under 3 minutes 0.22861243333897174\n",
            "distance km under 4 minutes 0.3105970057926929\n",
            "distance km under 5 minutes 0.721766709087411\n",
            "distance km under 6 minutes 0.9022671189166517\n",
            "distance km under 7 minutes 0.5885506297090981\n",
            "distance km under 8 minutes 0.8793035002255413\n",
            "distance km under 10 minutes 0.22534045063893088\n",
            "distance km under 11 minutes 0.6074797918965577\n",
            "distance km under 12 minutes 0.6332898128072468\n",
            "distance km under 13 minutes 0.16252106886816498\n",
            "distance km under 14 minutes 0.727510185106826\n",
            "distance km under 15 minutes 1.026274834571218\n",
            "distance km under 16 minutes 0.37187020175950464\n",
            "distance km under 19 minutes 0.13857892211474668\n",
            "distance km under 20 minutes 2.8050381331665646\n",
            "distance km under 29 minutes 2.569087033768754\n"
          ]
        }
      ]
    }
  ]
}